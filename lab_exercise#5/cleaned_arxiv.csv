"title","author","subject","abstract","date"
"algorithmic syntactic causal identification","dhurim cakiqi, max a. little","artificial intelligence","causal identification in causal bayes nets (cbns) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. however, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on cbns. however, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. we show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. in this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. this allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general id algorithm through fixing. our description is given entirely in terms of the non-parametric admg structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. we use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.",2024-03-14
"enabling waypoint generation for collaborative robots using llms and mixed reality","cathy mengying fang, krzysztof zieliński, pattie maes, joe paradiso, bruce blumberg, mikkel baun kjærgaard","human-computer interaction","programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. we propose a framework that simplifies robot deployment by allowing direct communication using natural language. it uses large language models (llm) for prompt processing, workspace understanding, and waypoint generation. it also employs augmented reality (ar) to provide visual feedback of the planned outcome. we showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).",2024-03-14
"bugs in large language models generated code","florian tambon, arghavan moradi dakhel, amin nikanjam, foutse khomh, michel c. desmarais, giuliano antoniol","software engineering","large language models (llms) for code have gained significant attention recently. they can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in software engineering (se), i.e., automatic code generation. similar to human-written code, llm-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. given the increasing adoption of llm-based code generation tools (e.g., github copilot) in se activities, it is critical to understand the characteristics of bugs contained in code generated by llms. this paper examines a sample of 333 bugs collected from code generated using three leading llms (i.e., codegen, pangu-coder, and codex) and identifies the following 10 distinctive bug patterns: misinterpretations, syntax error, silly mistake, prompt-biased code, missing corner case, wrong input type, hallucinated object, wrong attribute, incomplete generation, and non-prompted consideration. the bug patterns are presented in the form of a taxonomy. the identified bug patterns are validated using an online survey with 34 llm practitioners and researchers. the surveyed participants generally asserted the significance and prevalence of the bug patterns. researchers and practitioners can leverage these findings to develop effective quality assurance techniques for llm-generated code. this study sheds light on the distinctive characteristics of llm-generated code.",2024-03-13
"formalizing date arithmetic and statically detecting ambiguities for the law","raphaël monat, aymeric fromherz, denis merigoux","programming languages","legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. in this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. we first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the f* proof assistant. building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. we finally integrate our approach in the catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the catala implementation of the french housing benefits, leading to the discovery of several date-related ambiguities.",2024-03-13
"jaxbind: bind any function to jax","jakob roth, martin reinecke, gordian edenhofer","instrumentation and methods for astrophysics","jax is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into jax. reimplementing the existing code in jax is often impractical and the existing interface in jax for binding custom code requires deep knowledge of jax and its c++ backend. the goal of jaxbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to jax. specifically, jaxbind provides an easy-to-use python interface for defining custom so-called jax primitives that support arbitrary jax transformations.",2024-03-13
"predictive analysis of tuberculosis treatment outcomes using machine learning: a karnataka tb data study at a scale","seshasai nath chinagudaba, darshan gera, krishna kiran vamsi dasu, uma shankar s, kiran k, anil singarajpure, shivayogappa.u, somashekar n, vineet kumar chadda, sharath b n","machine learning","tuberculosis (tb) remains a global health threat, ranking among the leading causes of mortality worldwide. in this context, machine learning (ml) has emerged as a transformative force, providing innovative solutions to the complexities associated with tb treatment.this study explores how machine learning, especially with tabular data, can be used to predict tuberculosis (tb) treatment outcomes more accurately. it transforms this prediction task into a binary classification problem, generating risk scores from patient data sourced from nikshay, india's national tb control program, which includes over 500,000 patient records.
data preprocessing is a critical component of the study, and the model achieved an recall of 98% and an auc-roc score of 0.95 on the validation set, which includes 20,000 patient records.we also explore the use of natural language processing (nlp) for improved model learning. our results, corroborated by various metrics and ablation studies, validate the effectiveness of our approach. the study concludes by discussing the potential ramifications of our research on tb eradication efforts and proposing potential avenues for future work. this study marks a significant stride in the battle against tb, showcasing the potential of machine learning in healthcare.",2024-03-13
"devbench: a comprehensive benchmark for software development","bowen li, wenhan wu, ziwei tang, lin shi, john yang, jinyang li, shunyu yao, chen qian, binyuan hui, qicheng zhang, zhiyin yu, he du, ping yang, dahua lin, chao peng, kai chen","computation and language","recent advancements in large language models (llms) have significantly enhanced their coding capabilities. however, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. to this end, we propose devbench, a comprehensive benchmark that evaluates llms across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. devbench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. empirical studies show that current llms, including gpt-4-turbo, fail to solve the challenges presented within devbench. analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. our findings offer actionable insights for the future development of llms toward real-world programming applications. our benchmark is available at this https url",2024-03-13
"a picture is worth a thousand words: exploring diagram and video-based oop exercises to counter llm over-reliance","bruno pereira cipriano, pedro alves, paul denny","software engineering","much research has highlighted the impressive capabilities of large language models (llms), like gpt and bard, for solving introductory programming exercises. recent work has shown that llms can effectively solve a range of more complex object-oriented programming (oop) exercises with text-based specifications. this raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. to address this, we propose an innovative approach to formulating oop tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in oop courses. we introduce a novel notation system for specifying oop assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. student perceptions of this approach are explored through a survey (n=56). generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. this notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. furthermore, students reported being less inclined to rely on llm-based code generation tools for these diagram and video-based exercises. experiments with gpt-4 and bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions.",2024-03-13
"cleanagent: automating data standardization with llm-based agents","danrui qi, jiannan wang","machine learning","data standardization is a crucial part in data science life cycle. while tools like pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. although large language models (llms) like chatgpt have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. to solve these challenges, our key idea is to propose a python library with declarative, unified apis for standardizing column types, simplifying the code generation of llm with concise api calls. we first propose dataprep.clean which is written as a component of the dataprep library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. then we introduce the cleanagent framework integrating dataprep.clean and llm-based agents to automate the data standardization process. with cleanagent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.",2024-03-13
"mastering text, code and math simultaneously via fusing highly specialized language models","ning ding, yulin chen, ganqu cui, xingtai lv, ruobing xie, bowen zhou, zhiyuan liu, maosong sun","computation and language","underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (llms) that strive to achieve high performance across all three domains simultaneously. achieving a very high level of proficiency for an llm within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. in this paper, we propose to fuse models that are already highly-specialized directly. the proposed fusing framework, ultrafuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. a token-level gating mechanism is introduced to blend the specialists' outputs. a two-stage training strategy accompanied by balanced sampling is designed to ensure stability. to effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, ultrachat 2, which includes text, code, and mathematical content. this dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. experiments show that our model could simultaneously achieve mastery of the three crucial domains.",2024-03-13
"continuous object state recognition for cooking robots using pre-trained vision-language models and black-box optimization","kento kawaharazuka, naoaki kanazawa, yoshiki obinata, kei okada, masayuki inaba","robotics","the state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. on the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. in addition, the state changes of food are complex and cannot be easily described by manual programming. therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. by using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. we also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved. we demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying.",2024-03-13
"empowering robotics with large language models: osmag map comprehension with llms","fujing xie, sören schwertfeger","robotics","recently, large language models (llms) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. in this letter, we address the problem of enabling llms to comprehend area graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics. area graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. in contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmag (area graph in opensstreetmap format) is stored in a xml textual format naturally readable by llms. furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmag, facilitating this map representation comprehensible by llms, traditional robotic algorithms and humans. our experiments show that with a proper map representation, llms possess the capability to understand maps and answer queries based on that understanding. following simple fine-tuning of llama2 models, it surpassed chatgpt-3.5 in tasks involving topology and hierarchy understanding. our dataset, dataset generation code, fine-tuned lora adapters can be accessed at this https url.",2024-03-13
"merino: entropy-driven design for generative language models on iot devices","youpeng zhao, ming lin, huadong tang, qiang wu, jun wang","machine learning","generative large language models (llms) stand as a revolutionary advancement in the modern era of artificial intelligence (ai). however, directly deploying llms in resource-constrained hardware, such as internet-of-things (iot) devices, is difficult due to their high computational cost. in this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. the whole design procedure involves solving a mathematical programming (mp) problem, which can be done on the cpu within minutes, making it nearly zero-cost. we evaluate our designed models, termed merino, across nine nlp downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. notably, merino achieves similar or better zero performance compared to the 350m parameter opt while being 4.9x faster on nvidia jetson nano with 5.5x reduction in model size. code will be made available soon.",2024-02-28
"neural slot interpreters: grounding object semantics in emergent slot representations","bhishma dedhia, niraj k. jha","computer vision and pattern recognition","object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. however, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. we present the neural slot interpreter (nsi) that learns to ground and generate object semantics via slot representations. at the core of nsi is an xml-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. finally, we formulate the nsi program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. experiments on bi-modal retrieval tasks demonstrate the efficacy of the learned alignments, surpassing set-matching-based predictors by a significant margin. moreover, learning the program generator from grounded associations enhances the predictive power of slots. nsi generated programs demonstrate improved performance of object-centric learners on property prediction and object detection, and scale with real-world scene complexity.",2024-02-02
"exploring safety generalization challenges of large language models via code","qibing ren, chang gao, jing shao, junchi yan, xin tan, yu qiao, wai lam, lizhuang ma","computation and language","the rapid advancement of large language models (llms) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. while strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. this paper introduces codeattack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of llms. our comprehensive studies on state-of-the-art llms including gpt-4, claude-2, and llama-2 series reveal a common safety vulnerability of these models against code input: codeattack consistently bypasses the safety guardrails of all models more than 80% of the time. furthermore, we find that a larger distribution gap between codeattack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. these findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of llms.",2024-03-12
"proskill: a formal skill language for acting in robotics","félix ingrand (laas-cnrs, université de toulouse, toulouse, france)","robotics","acting is an important decisional function for autonomous robots. acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. while sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. this language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. we illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission.",2024-03-12
"couler: unified machine learning workflow optimization in cloud","xiaoda wang, yuan tang, tengda guo, bo sang, jingji wu, jian sha, ke zhang, jiang qian, mingjie tang","databases","machine learning (ml) has become ubiquitous, fueling data-driven applications across various organizations. contrary to the traditional perception of ml in research, ml workflows can be complex, resource-intensive, and time-consuming. expanding an ml workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. currently, numerous workflow engines are available (with over ten being widely recognized). this variety poses a challenge for end-users in terms of mastering different engine apis. while efforts have primarily focused on optimizing ml operations (mlops) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.
in this work, we design and implement couler, a system designed for unified ml workflow optimization in the cloud. our main insight lies in the ability to generate an ml workflow using natural language (nl) descriptions. we integrate large language models (llms) into workflow generation, and provide a unified programming interface for various workflow engines. this approach alleviates the need to understand various workflow engines' apis. moreover, couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. these enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. couler is extensively deployed in real-world production scenarios at ant group, handling approximately 22k workflows daily, and has successfully improved the cpu/memory utilization by more than 15% and the workflow completion rate by around 17%.",2024-03-12
"drplanner: diagnosis and repair of motion planners using large language models","yuanfei lin, chenran li, mingyu ding, masayoshi tomizuka, wei zhan, matthias althoff","robotics","motion planners are essential for the safe operation of automated vehicles across various scenarios. however, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. to tackle the aforementioned issues, we present drplanner, the first framework designed to automatically diagnose and repair motion planners using large language models. initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively.",2024-03-12
"from english to asic: hardware implementation with large language model","emil goh, maoyang xiang, i-chyn wey, t. hui teo","hardware architecture","in the realm of asic engineering, the landscape has been significantly reshaped by the rapid development of llm, paralleled by an increase in the complexity of modern digital circuits. this complexity has escalated the requirements for hdl coding, necessitating a higher degree of precision and sophistication. however, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. these challenges have highlighted the gap between the potential of llms to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. to address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the hdl code dataset has been developed. the fine-tuning aims to enhance models' proficiency in generating precise and efficient asic design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. the model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. this approach is expected to facilitate a simplified and more efficient llm-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of hdl coding and thus streamlining the asic development process.",2024-03-11
"deriving dependently-typed oop from first principles -- extended version with additional appendices","david binder, ingo skupin, tim süberkrüb, klaus ostermann","programming languages","the expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. when abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. the expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). this difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. in this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. that is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. our central contribution is a dependently typed calculus which contains two dual language fragments. we provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. we have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality.",2024-03-11
"automatic generation of python programs using context-free grammars","kamel yamani, marwa naïr, riyadh baghdadi","programming languages","in recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. however, procuring high-quality data remains challenging, especially for code. to address this, we developed tinypy generator, a tool that generates random python programs using a context-free grammar. the generated programs are guaranteed to be correct by construction. our system uses custom production rules (in the backus-naur form (bnf) format) to recursively generate code. this allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. our proposed tool enables effortless large-scale python code generation, beneficial for a wide range of applications. tinypy generator is particularly useful in the field of machine learning, where it can generate substantial amounts of python code for training python language models. additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of code interpreters or compilers. unlike existing research, we have open-sourced our implementation. this allows customization according to user needs and extends potential usage to other languages.",2024-03-11
"llms still can't avoid instanceof: an investigation into gpt-3.5, gpt-4 and bard's capacity to handle object-oriented programming assignments","bruno pereira cipriano, pedro alves","software engineering","large language models (llms) have emerged as promising tools to assist students while solving programming assignments. however, object-oriented programming (oop), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. contrary to introductory programming exercises, there exists a research gap with regard to the behavior of llms in oop contexts. in this study, we experimented with three prominent llms - gpt-3.5, gpt-4, and bard - to solve real-world oop exercises used in educational settings, subsequently validating their solutions using an automatic assessment tool (aat). the findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of oop. gpt-4 stood out as the most proficient, followed by gpt-3.5, with bard trailing last. we advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing llms with aats in pedagogical settings. in conclusion, while gpt-4 showcases promise, the deployment of these models in oop education still mandates supervision.",2024-03-10
"are llms ready for visualization?","pere-pau vázquez","human-computer interaction","generative models have received a lot of attention in many areas of academia and the industry. their capabilities span many areas, from the invention of images given a prompt to the generation of concrete code to solve a certain programming issue. these two paradigmatic cases fall within two distinct categories of requirements, ranging from ""creativity"" to ""precision"", as characterized by bing chat, which employs chatgpt-4 as its backbone. visualization practitioners and researchers have wondered to what end one of such systems could accomplish our work in a more efficient way. several works in the literature have utilized them for the creation of visualizations. and some tools such as lida, incorporate them as part of their pipeline. nevertheless, to the authors' knowledge, no systematic approach for testing their capabilities has been published, which includes both extensive and in-depth evaluation. our goal is to fill that gap with a systematic approach that analyzes three elements: whether large language models are capable of correctly generating a large variety of charts, what libraries they can deal with effectively, and how far we can go to configure individual charts. to achieve this objective, we initially selected a diverse set of charts, which are commonly utilized in data visualization. we then developed a set of generic prompts that could be used to generate them, and analyzed the performance of different llms and libraries. the results include both the set of prompts and the data sources, as well as an analysis of the performance with different configurations.",2024-03-10
"explaining code with a purpose: an integrated approach for developing code comprehension and prompting skills","paul denny, david h. smith iv, max fowler, james prather, brett a. becker, juho leinonen","human-computer interaction","reading, understanding and explaining code have traditionally been important skills for novices learning programming. as large language models (llms) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an llm. thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with llms. one effective way to develop and assess code comprehension ability is with ``explain in plain english'' (eipe) questions, where students succinctly explain the purpose of a fragment of code. however, grading eipe questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. in this paper, we explore a natural synergy between eipe questions and code-generating llms to overcome this limitation. we propose using an llm to generate code based on students' responses to eipe questions -- not only enabling eipe responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. we investigate this idea in an introductory programming course and report student success in creating effective prompts for solving eipe questions. we also examine student perceptions of this activity and how it influences their views on the use of llms for aiding and assessing learning.",2024-03-10
"a novel refactoring and semantic aware abstract syntax tree differencing tool and a benchmark for evaluating the accuracy of diff tools","pouria alikhanifard, nikolaos tsantalis","software engineering","software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. abstract syntax tree (ast) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. despite the notable improvements brought by ast diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible ast nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. we propose a novel ast diff tool based on refactoringminer that resolves all aforementioned limitations. first, we improved refactoringminer to increase its statement mapping accuracy, and then we developed an algorithm that generates ast diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by refactoringminer. to evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of ast node mappings, including 800 bug-fixing commits and 188 refactoring commits. our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.",2024-03-09
"unisparse: an intermediate language for general sparse format customization","jie liu, zhongyuan zhao, zijian ding, benjamin brock, hongbo rong, zhiru zhang","computation and language","the ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. these formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. however, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. to overcome this deficiency, we propose unisparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. unlike the existing attribute-based frameworks, unisparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. as a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. we also develop a compiler leveraging the mlir infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. we demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an intel cpu, an nvidia gpu, an amd xilinx fpga, and a simulated processing-in-memory (pim) device.",2024-03-09
"we know i know you know; choreographic programming with multicast and multiply located values","mako bates, joseph p. near","programming languages","concurrent distributed systems are notoriously difficult to construct and reason about. choreographic programming is a recent paradigm that describes a distributed system in a single global program called a choreography. choreographies simplify reasoning about distributed systems and can ensure deadlock freedom by static analysis. in previous choreographic programming languages, each value is located at a single party, and the programmer is expected to insert special untyped ""select"" operations to ensure that all parties follow the same communication pattern.
we present he-lambda-small, a new choreographic programming language with multiply located values. he-lambda-small allows multicasting to a set of parties, and the resulting value will be located at all of them. this approach enables a simple and elegant alternative to ""select"": he-lambda-small requires that the guard for a conditional be located at all of the relevant parties. in he-lambda-small, checking that a choreography is well-typed suffices to show that it is deadlock-free. we present several case studies that demonstrate the use of multiply-located values to concisely encode tricky communication patterns described in previous work without the use of ""select"" or redundant communication.",2024-03-08
"watchat: explaining perplexing programs by debugging mental models","kartik chandra, tzu-mao li, rachit nigam, joshua tenenbaum, jonathan ragan-kelley","programming languages","often, a good explanation for a program's unexpected behavior is a bug in the programmer's code. but sometimes, an even better explanation is a bug in the programmer's mental model of the language they are using. instead of merely debugging our current code (""giving the programmer a fish""), what if our tools could directly debug our mental models (""teaching the programmer to fish"")? in this paper, we apply ideas from computational cognitive science to do exactly that. given a perplexing program, we use program synthesis techniques to automatically infer potential misconceptions that might cause the user to be surprised by the program's behavior. by analyzing these misconceptions, we provide succinct, useful explanations of the program's behavior. our methods can even be inverted to synthesize pedagogical example programs for diagnosing and correcting misconceptions in students.",2024-03-08
"llm4decompile: decompiling binary code with large language models","hanzhuo tan, qi luo, jing li, yuqun zhang","programming languages","decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. large language models (llms) show promise for programming tasks, motivating their application to decompilation. however, there does not exist any open-source llm for decompilation. moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. therefore, we release the first open-access decompilation llms ranging from 1b to 33b pre-trained on 4 billion tokens of c source code and the corresponding assembly code. the open-source llms can serve as baselines for further development in the field. to ensure practical program evaluation, we introduce decompile-eval, the first dataset that considers re-compilability and re-executability for decompilation. the benchmark emphasizes the importance of evaluating the decompilation model from the perspective of program semantics. experiments indicate that our llm4decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over gpt-4. our code, dataset, and models are released at this https url",2024-03-08
"dt-sim: property-based testing for mpc security","mako bates, joseph p. near","cryptography and security","formal methods for guaranteeing that a protocol satisfies a cryptographic security definition have advanced substantially, but such methods are still labor intensive and the need remains for an automated tool that can positively identify an insecure protocol. in this work, we demonstrate that property-based testing, ""run it a bunch of times and see if it breaks"", is effective for detecting security bugs in secure protocols. we specifically target secure multi-party computation (mpc), because formal methods targeting this security definition for bit-model implementations are particularly difficult. using results from the literature for probabilistic programming languages and statistical inference, we devise a test that can detect various flaws in a bit-level implementation of an mpc protocol. the test is grey-box; it requires only transcripts of randomness consumed by the protocol and of the inputs, outputs, and messages. it successfully detects several different mistakes and biases introduced into two different implementations of the classic gmw protocol. applied to hundreds of randomly generated protocols, it identifies nearly all of them as insecure. we also include an analysis of the parameters of the test, and discussion of what makes detection of mpc (in)security difficult.",2024-03-08
"automating the information extraction from semi-structured interview transcripts","angelina parfenova","computation and language","this paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of bert embeddings and hdbscan clustering. we present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. this tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.",2024-03-07
"evaluation of llms on syntax-aware code fill-in-the-middle tasks","linyuan gong, sida wang, mostafa elhoushi, alvin cheung","computation and language","we introduce syntax-aware fill-in-the-middle (safim), a new benchmark for evaluating large language models (llms) on the code fill-in-the-middle (fim) task. this benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after april 2022 to minimize data contamination. safim provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across llms. our comprehensive evaluation of 15 llms shows that fim pretraining not only enhances fim proficiency but also improves left-to-right (l2r) inference using llms. our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. safim thus serves as a foundational platform for future research in effective pretraining strategies for code llms. the evaluation toolkit and dataset are available at this https url, and the leaderboard is available at this https url.",2024-03-07
"quantifying contamination in evaluating code generation capabilities of language models","martin riddell, ansong ni, arman cohan","software engineering","while large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. while recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of llms in programming contexts. in this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. in our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. we also conduct extensive analysis on the factors that affects model memorization and generalization, such as model size, problem difficulty, and question length. we release all resulting files from our matching pipeline for future research.",2024-03-06
"online training of large language models: learn while chatting","juhao liang, ziwei wang, zhuoheng ma, jianquan li, zhiyi zhang, xiangbo wu, benyou wang","computation and language","large language models(llms) have dramatically revolutionized the field of natural language processing(nlp), offering remarkable capabilities that have garnered widespread usage. however, existing interaction paradigms between llms and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. this inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. to overcome these challenges, this paper introduces a novel interaction paradigm-'online training using external interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as ai agents or online/offline knowledge bases.",2024-03-04
"qrtree -- decision tree dialect specification of qrscript","stefano scanzio, matteo rosani, mattia scamuzzi, gianluca cena","networking and internet architecture","this specification document specifies the syntax and semantics of qrtree, which is a specific dialect of qrscript particularly suited to represent decision trees without chance nodes. the term dialect identifies one of the possible sub-languages that can be encoded inside of an eqr code via qrscript. this specification will describe an intermediate representation of qrtree, made through a language derived by the three-address code. it will then define the transformation rules from the intermediate representation to a binary code. the latter is a binary representation called eqrtreebytecode. these rules can also be applied inversely to transform the eqrtreebytecode into the intermediate representation. this specification document will pay particular attention to the creation of a compact eqrtreebytecode, as the maximum number of bits that can be stored in a qr code is, at the time of writing, equal to 2953 bytes (in the case of qr code version 40 with a ""low"" error correction level).",2024-03-07
"cedar: a new language for expressive, fast, safe, and analyzable authorization (extended version)","joseph w. cutler, craig disselkoen, aaron eline, shaobo he, kyle headley, michael hicks, kesha hietala, eleftherios ioannidis, john kastner, anwar mamat, darin mcadams, matt mccutchen, neha rungta, emina torlak, andrew wells","programming languages","cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. rather than embed authorization logic in an application's code, developers can write that logic as cedar policies and delegate access decisions to cedar's evaluation engine. cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. cedar's policy structure enables access requests to be decided quickly. cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized permissions do not change. we have modeled cedar in the lean programming language, and used lean's proof assistant to prove important properties of cedar's design. we have implemented cedar in rust, and released it open-source. comparing cedar to two open-source languages, openfga and rego, we find (subjectively) that cedar has equally or more readable policies, but (objectively) performs far better.",2024-03-07
"message-observing sessions","ryan kavanagh, brigitte pientka","programming languages","we present most, a process language with message-observing session types. message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels. hence, most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way. we give most a semantic foundation using traces with binding, a semantic approach for compositionally reasoning about traces in the presence of name generation. we use this semantics to prove type soundness and compositionality for most processes. we see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes.",2024-03-07
"strong priority and determinacy in timed ccs","luigi liquori, michael mendler","programming languages","building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called ""sequentially constructive reduction"" which is designed to capture the essence of synchronous programming. the distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. in particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language esterel. in the technical setting of ccs extended by clocks and priorities, we prove for a large class of processes, which we call ""structurally coherent"" the confluence property for constructive reductions. we further show that under some syntactic restrictions, called ""pivotable"" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. this covers a strictly larger class of processes compared to those that are confluent in milner's classical theory of ccs without priorities.",2024-03-07
"towards automatic composition of asp programs from natural language specifications","manuel borroto, irfan kareem, francesco ricca","artificial intelligence","this paper moves the first step towards automating the composition of answer set programming (asp) specifications. in particular, the following contributions are provided: (i) a dataset focused on graph-related problem specifications, designed to develop and assess tools for asp automatic coding; (ii) a two-step architecture, implemented in the nl2asp tool, for generating asp programs from natural language specifications. nl2asp uses neural machine translation to transform natural language into controlled natural language (cnl) statements. subsequently, cnl statements are converted into asp code using the cnl2asp tool. an experiment confirms the viability of the approach.",2024-03-07
"feedback-generation for programming exercises with gpt-4","imen azaiz, natalie kiesler, sven strickroth","artificial intelligence","ever since large language models (llms) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. llms such as codex, gpt-3.5, and gpt 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. this paper explores the quality of gpt-4 turbo's generated output for prompts containing both the programming task specification and a student's submission as input. two assignments from an introductory programming course were selected, and gpt-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. the output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. compared to prior work and analyses of gpt-3.5, gpt-4 turbo shows notable improvements. for example, the output is more structured and consistent. gpt-4 turbo can also accurately identify invalid casing in student programs' output. in some cases, the feedback also includes the output of the student program. at the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. the present work increases our understanding of llms' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on gpt-4.",2024-03-07
"whodunit: classifying code as human authored or gpt-4 generated -- a case study on codechef problems","oseremen joy idialu, noble saji mathews, rungroj maipradit, joanne m. atlee, mei nagappan","software engineering","artificial intelligence (ai) assistants such as github copilot and chatgpt, built on large language models like gpt-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative ai models. such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit ai generated code as their own work. our research explores the viability of using code stylometry and machine learning to distinguish between gpt-4 generated and human-authored code. our dataset comprises human-authored solutions from codechef and ai-authored solutions generated by gpt-4. our classifier outperforms baselines, with an f1-score and auc-roc score of 0.91. a variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an f1-score and auc-roc score of 0.89. we also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. our study shows that code stylometry is a promising approach for distinguishing between gpt-4 generated code and human-authored code.",2024-03-06
"guiding enumerative program synthesis with large language models","yixuan li, julian parsert, elizabeth polgreen","artificial intelligence","pre-trained large language models (llms) are beginning to dominate the discourse around automatic code generation with natural language specifications. in contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. in this paper, we evaluate the abilities of llms to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. when one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an llm into a weighted probabilistic search. this allows the synthesizer to provide the llm with information about the progress of the enumerator, and the llm to provide the enumerator with syntactic guidance in an iterative loop. we evaluate our techniques on benchmarks from the syntax-guided synthesis (sygus) competition. we find that gpt-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the llm into an enumerative synthesis algorithm shows significant performance gains over both the llm and the enumerative synthesizer alone and the winning sygus competition tool.",2024-03-06
"identify critical nodes in complex network with large language models","jinzhu mao, dongyun zou, li sheng, siyi liu, chen gao, yue wang, yong li","social and information networks","identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. therefore, we propose an approach that empowers evolutionary algorithm (ea) with large language models (llms), to generate a function called ""score\_nodes"" which can further be used to identify crucial nodes based on their assigned scores. our model consists of three main components: manual initialization, population management, and llms-based evolution. it evolves from initial populations with a set of designed node scoring functions created manually. llms leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. these functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. extensive experiments demonstrate the excellent performance of our method, showcasing its strong generalization ability compared to other state-of-the-art algorithms. it can consistently and orderly generate diverse and efficient node scoring functions. all source codes and models that can reproduce all results in this work are publicly available at this link: \url{https://anonymous.4open.science/r/llm4cn-6520}",2024-03-01
"ircoder: intermediate representations make language models robust multilingual code generators","indraneil paul, jun luo, goran glavaš, iryna gurevych","artificial intelligence","code understanding and generation have fast become some of the most popular applications of language models (lms). nonetheless, research on multilingual aspects of code-lms (i.e., lms for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc lm adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. in particular, most mainstream code-lms have been pre-trained on source code files alone. in this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of code-lms and facilitate cross-lingual transfer.
to this end, we first compile sltrans, a parallel dataset consisting of nearly 4m self-contained source code files coupled with respective intermediate representations. next, starting from various base code-lms (ranging in size from 1.1b to 7.3b parameters), we carry out continued causal language modelling training on sltrans, forcing the code-lms to (1) learn the ir language and (2) align the ir constructs with respective constructs of various programming languages. our resulting models, dubbed ircoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.",2024-03-06
"digitality as a ""longue durèe"" historical phenomenon","salvatore spina","computers and society","the digital age introduced the digital ecological niche (den), revolutionizing human interactions. the advent of digital history (dhy) has marked a methodological shift in historical studies, tracing its roots to babbage and lovelace's 19th-century work on ""coding"" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed ""person2persons2machines."" this evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by babbage and lovelace's contributions to mathematical linguistic systems, laying the groundwork for computer science. this field, central to 20th-century mainframe interaction through programming languages and formalization, situates digital history within a broader historical context. here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. nonetheless, the extent to which computation and turing machines can fully understand and interpret history remains a subject of debate.",2024-03-06
"robust mitl planning under uncertain navigation times","alexis linard, anna gautier, daniel duberg, jana tumova","robotics","in environments like offices, the duration of a robot's navigation between two locations may vary over time. for instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. in this work, we address the problem of routing in such environments with tasks expressed in metric interval temporal logic (mitl) - a rich robot task specification language that allows us to capture explicit time requirements. our objective is to find a strategy that maximizes the temporal robustness of the robot's mitl task. as the first step towards a solution, we define a mixed-integer linear programming approach to solving the task planning problem over a varying weighted transition system, where navigation durations are deterministic but vary depending on the time of day. then, we apply this planner to optimize for mitl temporal robustness in markov decision processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. finally, we develop a receding horizon planner for markov decision processes that preserves guarantees over mitl temporal robustness. we show the scalability of our planning algorithms in simulations of robotic tasks.",2024-03-06
"automatic bi-modal question title generation for stack overflow with prompt learning","shaoyu yang, xiang chen, ke liu, guang yang, chi yu","software engineering","when drafting question posts for stack overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. therefore, improving the quality of question titles has attracted the wide attention of researchers. an initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. however, this study ignored the helpful information in their corresponding problem descriptions. therefore, we propose an approach sotitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. later we fine-tune the pre-trained language model codet5 to automatically generate the titles. unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. to solve this issue, sotitle+ further prompt-tunes codet5 with hybrid prompts (i.e., mixture of hard and soft prompts). to verify the effectiveness of sotitle+, we construct a large-scale high-quality corpus from recent data dumps shared by stack overflow. our corpus includes 179,119 high-quality question posts for six popular programming languages. experimental results show that sotitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. our work indicates that considering bi-modal information and prompt learning in stack overflow title generation is a promising exploration direction.",2024-03-06
"magic markup: maintaining document-external markup with an llm","edward misback, zachary tatlock, steven l. tanimoto","computation and language","text documents, including programs, typically have human-readable semantic structure. historically, programmatic access to these semantics has required explicit in-document tagging. especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly. today, language models offer a new method: metadata can be bound to entities in changing text using a model's human-like understanding of semantics, with no requirements on the document structure. this method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation. we contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves. we also contribute a formal problem definition, an empirical synthetic benchmark suite, and our benchmark generator. our system achieves an accuracy of 90% on our benchmarks and can replace a document's tags in parallel at a rate of 5 seconds per tag. while there remains significant room for improvement, we find performance reliable enough to justify further exploration of applications.",2024-03-06
"generative explanations for program synthesizers","amirmohammad nazari, souti chattopadhyay, swabha swayamdipta, mukund raghothaman","programming languages","despite great advances in program synthesis techniques, they remain algorithmic black boxes. although they guarantee that when synthesis is successful, the implementation satisfies the specification, they provide no additional information regarding how the implementation works or the manner in which the specification is realized. one possibility to answer these questions is to use large language models (llms) to construct human-readable explanations. unfortunately, experiments reveal that llms frequently produce nonsensical or misleading explanations when applied to the unidiomatic code produced by program synthesizers.
in this paper, we develop an approach to reliably augment the implementation with explanatory names. we recover fine-grained input-output data from the synthesis algorithm to enhance the prompt supplied to the llm, and use a combination of a program verifier and a second language model to validate the proposed explanations before presenting them to the user. together, these techniques massively improve the accuracy of the proposed names, from 24% to 79% respectively. through a pair of small user studies, we find that users significantly prefer the explanations produced by our technique (76% of responses indicating the appropriateness of the presenting names) to the baseline (with only 2% of responses approving of the suggestions), and that the proposed names measurably help users in understanding the synthesized implementation.",2024-03-06
"explaining genetic programming trees using large language models","paula maddigan, andrew lensen, bing xue","neural and evolutionary computing","genetic programming (gp) has the potential to generate explainable results, especially when used for dimensionality reduction. in this research, we investigate the potential of leveraging explainable ai (xai) and large language models (llms) like chatgpt to improve the interpretability of gp-based non-linear dimensionality reduction. our study introduces a novel xai dashboard named gp4nldr, the first approach to combine state-of-the-art gp with an llm-powered chatbot to provide comprehensive, user-centred explanations. we showcase the system's ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies. our study highlights the importance of prompt engineering in eliciting accurate and pertinent responses from llms. we also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in generative ai. our findings demonstrate its potential in advancing the explainability of gp algorithms. this opens the door for future research into explaining gp models with llms.",2024-03-06
"algorithmic syntactic causal identification","dhurim cakiqi, max a. little","artificial intelligence","causal identification in causal bayes nets (cbns) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. however, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on cbns. however, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. we show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. in this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. this allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general id algorithm through fixing. our description is given entirely in terms of the non-parametric admg structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. we use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.",2024-03-14
"enabling waypoint generation for collaborative robots using llms and mixed reality","cathy mengying fang, krzysztof zieliński, pattie maes, joe paradiso, bruce blumberg, mikkel baun kjærgaard","human-computer interaction","programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. we propose a framework that simplifies robot deployment by allowing direct communication using natural language. it uses large language models (llm) for prompt processing, workspace understanding, and waypoint generation. it also employs augmented reality (ar) to provide visual feedback of the planned outcome. we showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).",2024-03-14
"bugs in large language models generated code","florian tambon, arghavan moradi dakhel, amin nikanjam, foutse khomh, michel c. desmarais, giuliano antoniol","software engineering","large language models (llms) for code have gained significant attention recently. they can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in software engineering (se), i.e., automatic code generation. similar to human-written code, llm-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. given the increasing adoption of llm-based code generation tools (e.g., github copilot) in se activities, it is critical to understand the characteristics of bugs contained in code generated by llms. this paper examines a sample of 333 bugs collected from code generated using three leading llms (i.e., codegen, pangu-coder, and codex) and identifies the following 10 distinctive bug patterns: misinterpretations, syntax error, silly mistake, prompt-biased code, missing corner case, wrong input type, hallucinated object, wrong attribute, incomplete generation, and non-prompted consideration. the bug patterns are presented in the form of a taxonomy. the identified bug patterns are validated using an online survey with 34 llm practitioners and researchers. the surveyed participants generally asserted the significance and prevalence of the bug patterns. researchers and practitioners can leverage these findings to develop effective quality assurance techniques for llm-generated code. this study sheds light on the distinctive characteristics of llm-generated code.",2024-03-13
"formalizing date arithmetic and statically detecting ambiguities for the law","raphaël monat, aymeric fromherz, denis merigoux","programming languages","legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. in this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. we first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the f* proof assistant. building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. we finally integrate our approach in the catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the catala implementation of the french housing benefits, leading to the discovery of several date-related ambiguities.",2024-03-13
"jaxbind: bind any function to jax","jakob roth, martin reinecke, gordian edenhofer","instrumentation and methods for astrophysics","jax is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into jax. reimplementing the existing code in jax is often impractical and the existing interface in jax for binding custom code requires deep knowledge of jax and its c++ backend. the goal of jaxbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to jax. specifically, jaxbind provides an easy-to-use python interface for defining custom so-called jax primitives that support arbitrary jax transformations.",2024-03-13
"predictive analysis of tuberculosis treatment outcomes using machine learning: a karnataka tb data study at a scale","seshasai nath chinagudaba, darshan gera, krishna kiran vamsi dasu, uma shankar s, kiran k, anil singarajpure, shivayogappa.u, somashekar n, vineet kumar chadda, sharath b n","machine learning","tuberculosis (tb) remains a global health threat, ranking among the leading causes of mortality worldwide. in this context, machine learning (ml) has emerged as a transformative force, providing innovative solutions to the complexities associated with tb treatment.this study explores how machine learning, especially with tabular data, can be used to predict tuberculosis (tb) treatment outcomes more accurately. it transforms this prediction task into a binary classification problem, generating risk scores from patient data sourced from nikshay, india's national tb control program, which includes over 500,000 patient records.
data preprocessing is a critical component of the study, and the model achieved an recall of 98% and an auc-roc score of 0.95 on the validation set, which includes 20,000 patient records.we also explore the use of natural language processing (nlp) for improved model learning. our results, corroborated by various metrics and ablation studies, validate the effectiveness of our approach. the study concludes by discussing the potential ramifications of our research on tb eradication efforts and proposing potential avenues for future work. this study marks a significant stride in the battle against tb, showcasing the potential of machine learning in healthcare.",2024-03-13
"devbench: a comprehensive benchmark for software development","bowen li, wenhan wu, ziwei tang, lin shi, john yang, jinyang li, shunyu yao, chen qian, binyuan hui, qicheng zhang, zhiyin yu, he du, ping yang, dahua lin, chao peng, kai chen","computation and language","recent advancements in large language models (llms) have significantly enhanced their coding capabilities. however, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. to this end, we propose devbench, a comprehensive benchmark that evaluates llms across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. devbench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. empirical studies show that current llms, including gpt-4-turbo, fail to solve the challenges presented within devbench. analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. our findings offer actionable insights for the future development of llms toward real-world programming applications. our benchmark is available at this https url",2024-03-13
"a picture is worth a thousand words: exploring diagram and video-based oop exercises to counter llm over-reliance","bruno pereira cipriano, pedro alves, paul denny","software engineering","much research has highlighted the impressive capabilities of large language models (llms), like gpt and bard, for solving introductory programming exercises. recent work has shown that llms can effectively solve a range of more complex object-oriented programming (oop) exercises with text-based specifications. this raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. to address this, we propose an innovative approach to formulating oop tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in oop courses. we introduce a novel notation system for specifying oop assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. student perceptions of this approach are explored through a survey (n=56). generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. this notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. furthermore, students reported being less inclined to rely on llm-based code generation tools for these diagram and video-based exercises. experiments with gpt-4 and bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions.",2024-03-13
"cleanagent: automating data standardization with llm-based agents","danrui qi, jiannan wang","machine learning","data standardization is a crucial part in data science life cycle. while tools like pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. although large language models (llms) like chatgpt have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. to solve these challenges, our key idea is to propose a python library with declarative, unified apis for standardizing column types, simplifying the code generation of llm with concise api calls. we first propose dataprep.clean which is written as a component of the dataprep library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. then we introduce the cleanagent framework integrating dataprep.clean and llm-based agents to automate the data standardization process. with cleanagent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.",2024-03-13
"mastering text, code and math simultaneously via fusing highly specialized language models","ning ding, yulin chen, ganqu cui, xingtai lv, ruobing xie, bowen zhou, zhiyuan liu, maosong sun","computation and language","underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (llms) that strive to achieve high performance across all three domains simultaneously. achieving a very high level of proficiency for an llm within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. in this paper, we propose to fuse models that are already highly-specialized directly. the proposed fusing framework, ultrafuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. a token-level gating mechanism is introduced to blend the specialists' outputs. a two-stage training strategy accompanied by balanced sampling is designed to ensure stability. to effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, ultrachat 2, which includes text, code, and mathematical content. this dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. experiments show that our model could simultaneously achieve mastery of the three crucial domains.",2024-03-13
"continuous object state recognition for cooking robots using pre-trained vision-language models and black-box optimization","kento kawaharazuka, naoaki kanazawa, yoshiki obinata, kei okada, masayuki inaba","robotics","the state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. on the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. in addition, the state changes of food are complex and cannot be easily described by manual programming. therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. by using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. we also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved. we demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying.",2024-03-13
"empowering robotics with large language models: osmag map comprehension with llms","fujing xie, sören schwertfeger","robotics","recently, large language models (llms) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. in this letter, we address the problem of enabling llms to comprehend area graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics. area graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. in contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmag (area graph in opensstreetmap format) is stored in a xml textual format naturally readable by llms. furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmag, facilitating this map representation comprehensible by llms, traditional robotic algorithms and humans. our experiments show that with a proper map representation, llms possess the capability to understand maps and answer queries based on that understanding. following simple fine-tuning of llama2 models, it surpassed chatgpt-3.5 in tasks involving topology and hierarchy understanding. our dataset, dataset generation code, fine-tuned lora adapters can be accessed at this https url.",2024-03-13
"merino: entropy-driven design for generative language models on iot devices","youpeng zhao, ming lin, huadong tang, qiang wu, jun wang","machine learning","generative large language models (llms) stand as a revolutionary advancement in the modern era of artificial intelligence (ai). however, directly deploying llms in resource-constrained hardware, such as internet-of-things (iot) devices, is difficult due to their high computational cost. in this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. the whole design procedure involves solving a mathematical programming (mp) problem, which can be done on the cpu within minutes, making it nearly zero-cost. we evaluate our designed models, termed merino, across nine nlp downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. notably, merino achieves similar or better zero performance compared to the 350m parameter opt while being 4.9x faster on nvidia jetson nano with 5.5x reduction in model size. code will be made available soon.",2024-02-28
"neural slot interpreters: grounding object semantics in emergent slot representations","bhishma dedhia, niraj k. jha","computer vision and pattern recognition","object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. however, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. we present the neural slot interpreter (nsi) that learns to ground and generate object semantics via slot representations. at the core of nsi is an xml-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. finally, we formulate the nsi program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. experiments on bi-modal retrieval tasks demonstrate the efficacy of the learned alignments, surpassing set-matching-based predictors by a significant margin. moreover, learning the program generator from grounded associations enhances the predictive power of slots. nsi generated programs demonstrate improved performance of object-centric learners on property prediction and object detection, and scale with real-world scene complexity.",2024-02-02
"exploring safety generalization challenges of large language models via code","qibing ren, chang gao, jing shao, junchi yan, xin tan, yu qiao, wai lam, lizhuang ma","computation and language","the rapid advancement of large language models (llms) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. while strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. this paper introduces codeattack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of llms. our comprehensive studies on state-of-the-art llms including gpt-4, claude-2, and llama-2 series reveal a common safety vulnerability of these models against code input: codeattack consistently bypasses the safety guardrails of all models more than 80% of the time. furthermore, we find that a larger distribution gap between codeattack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. these findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of llms.",2024-03-12
"proskill: a formal skill language for acting in robotics","félix ingrand (laas-cnrs, université de toulouse, toulouse, france)","robotics","acting is an important decisional function for autonomous robots. acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. while sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. this language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. we illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission.",2024-03-12
"couler: unified machine learning workflow optimization in cloud","xiaoda wang, yuan tang, tengda guo, bo sang, jingji wu, jian sha, ke zhang, jiang qian, mingjie tang","databases","machine learning (ml) has become ubiquitous, fueling data-driven applications across various organizations. contrary to the traditional perception of ml in research, ml workflows can be complex, resource-intensive, and time-consuming. expanding an ml workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. currently, numerous workflow engines are available (with over ten being widely recognized). this variety poses a challenge for end-users in terms of mastering different engine apis. while efforts have primarily focused on optimizing ml operations (mlops) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.
in this work, we design and implement couler, a system designed for unified ml workflow optimization in the cloud. our main insight lies in the ability to generate an ml workflow using natural language (nl) descriptions. we integrate large language models (llms) into workflow generation, and provide a unified programming interface for various workflow engines. this approach alleviates the need to understand various workflow engines' apis. moreover, couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. these enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. couler is extensively deployed in real-world production scenarios at ant group, handling approximately 22k workflows daily, and has successfully improved the cpu/memory utilization by more than 15% and the workflow completion rate by around 17%.",2024-03-12
"drplanner: diagnosis and repair of motion planners using large language models","yuanfei lin, chenran li, mingyu ding, masayoshi tomizuka, wei zhan, matthias althoff","robotics","motion planners are essential for the safe operation of automated vehicles across various scenarios. however, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. to tackle the aforementioned issues, we present drplanner, the first framework designed to automatically diagnose and repair motion planners using large language models. initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively.",2024-03-12
"from english to asic: hardware implementation with large language model","emil goh, maoyang xiang, i-chyn wey, t. hui teo","hardware architecture","in the realm of asic engineering, the landscape has been significantly reshaped by the rapid development of llm, paralleled by an increase in the complexity of modern digital circuits. this complexity has escalated the requirements for hdl coding, necessitating a higher degree of precision and sophistication. however, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. these challenges have highlighted the gap between the potential of llms to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. to address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the hdl code dataset has been developed. the fine-tuning aims to enhance models' proficiency in generating precise and efficient asic design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. the model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. this approach is expected to facilitate a simplified and more efficient llm-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of hdl coding and thus streamlining the asic development process.",2024-03-11
"deriving dependently-typed oop from first principles -- extended version with additional appendices","david binder, ingo skupin, tim süberkrüb, klaus ostermann","programming languages","the expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. when abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. the expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). this difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. in this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. that is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. our central contribution is a dependently typed calculus which contains two dual language fragments. we provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. we have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality.",2024-03-11
"automatic generation of python programs using context-free grammars","kamel yamani, marwa naïr, riyadh baghdadi","programming languages","in recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. however, procuring high-quality data remains challenging, especially for code. to address this, we developed tinypy generator, a tool that generates random python programs using a context-free grammar. the generated programs are guaranteed to be correct by construction. our system uses custom production rules (in the backus-naur form (bnf) format) to recursively generate code. this allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. our proposed tool enables effortless large-scale python code generation, beneficial for a wide range of applications. tinypy generator is particularly useful in the field of machine learning, where it can generate substantial amounts of python code for training python language models. additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of code interpreters or compilers. unlike existing research, we have open-sourced our implementation. this allows customization according to user needs and extends potential usage to other languages.",2024-03-11
"llms still can't avoid instanceof: an investigation into gpt-3.5, gpt-4 and bard's capacity to handle object-oriented programming assignments","bruno pereira cipriano, pedro alves","software engineering","large language models (llms) have emerged as promising tools to assist students while solving programming assignments. however, object-oriented programming (oop), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. contrary to introductory programming exercises, there exists a research gap with regard to the behavior of llms in oop contexts. in this study, we experimented with three prominent llms - gpt-3.5, gpt-4, and bard - to solve real-world oop exercises used in educational settings, subsequently validating their solutions using an automatic assessment tool (aat). the findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of oop. gpt-4 stood out as the most proficient, followed by gpt-3.5, with bard trailing last. we advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing llms with aats in pedagogical settings. in conclusion, while gpt-4 showcases promise, the deployment of these models in oop education still mandates supervision.",2024-03-10
"are llms ready for visualization?","pere-pau vázquez","human-computer interaction","generative models have received a lot of attention in many areas of academia and the industry. their capabilities span many areas, from the invention of images given a prompt to the generation of concrete code to solve a certain programming issue. these two paradigmatic cases fall within two distinct categories of requirements, ranging from ""creativity"" to ""precision"", as characterized by bing chat, which employs chatgpt-4 as its backbone. visualization practitioners and researchers have wondered to what end one of such systems could accomplish our work in a more efficient way. several works in the literature have utilized them for the creation of visualizations. and some tools such as lida, incorporate them as part of their pipeline. nevertheless, to the authors' knowledge, no systematic approach for testing their capabilities has been published, which includes both extensive and in-depth evaluation. our goal is to fill that gap with a systematic approach that analyzes three elements: whether large language models are capable of correctly generating a large variety of charts, what libraries they can deal with effectively, and how far we can go to configure individual charts. to achieve this objective, we initially selected a diverse set of charts, which are commonly utilized in data visualization. we then developed a set of generic prompts that could be used to generate them, and analyzed the performance of different llms and libraries. the results include both the set of prompts and the data sources, as well as an analysis of the performance with different configurations.",2024-03-10
"explaining code with a purpose: an integrated approach for developing code comprehension and prompting skills","paul denny, david h. smith iv, max fowler, james prather, brett a. becker, juho leinonen","human-computer interaction","reading, understanding and explaining code have traditionally been important skills for novices learning programming. as large language models (llms) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an llm. thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with llms. one effective way to develop and assess code comprehension ability is with ``explain in plain english'' (eipe) questions, where students succinctly explain the purpose of a fragment of code. however, grading eipe questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. in this paper, we explore a natural synergy between eipe questions and code-generating llms to overcome this limitation. we propose using an llm to generate code based on students' responses to eipe questions -- not only enabling eipe responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. we investigate this idea in an introductory programming course and report student success in creating effective prompts for solving eipe questions. we also examine student perceptions of this activity and how it influences their views on the use of llms for aiding and assessing learning.",2024-03-10
"a novel refactoring and semantic aware abstract syntax tree differencing tool and a benchmark for evaluating the accuracy of diff tools","pouria alikhanifard, nikolaos tsantalis","software engineering","software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. abstract syntax tree (ast) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. despite the notable improvements brought by ast diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible ast nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. we propose a novel ast diff tool based on refactoringminer that resolves all aforementioned limitations. first, we improved refactoringminer to increase its statement mapping accuracy, and then we developed an algorithm that generates ast diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by refactoringminer. to evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of ast node mappings, including 800 bug-fixing commits and 188 refactoring commits. our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.",2024-03-09
"unisparse: an intermediate language for general sparse format customization","jie liu, zhongyuan zhao, zijian ding, benjamin brock, hongbo rong, zhiru zhang","computation and language","the ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. these formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. however, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. to overcome this deficiency, we propose unisparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. unlike the existing attribute-based frameworks, unisparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. as a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. we also develop a compiler leveraging the mlir infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. we demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an intel cpu, an nvidia gpu, an amd xilinx fpga, and a simulated processing-in-memory (pim) device.",2024-03-09
"we know i know you know; choreographic programming with multicast and multiply located values","mako bates, joseph p. near","programming languages","concurrent distributed systems are notoriously difficult to construct and reason about. choreographic programming is a recent paradigm that describes a distributed system in a single global program called a choreography. choreographies simplify reasoning about distributed systems and can ensure deadlock freedom by static analysis. in previous choreographic programming languages, each value is located at a single party, and the programmer is expected to insert special untyped ""select"" operations to ensure that all parties follow the same communication pattern.
we present he-lambda-small, a new choreographic programming language with multiply located values. he-lambda-small allows multicasting to a set of parties, and the resulting value will be located at all of them. this approach enables a simple and elegant alternative to ""select"": he-lambda-small requires that the guard for a conditional be located at all of the relevant parties. in he-lambda-small, checking that a choreography is well-typed suffices to show that it is deadlock-free. we present several case studies that demonstrate the use of multiply-located values to concisely encode tricky communication patterns described in previous work without the use of ""select"" or redundant communication.",2024-03-08
"watchat: explaining perplexing programs by debugging mental models","kartik chandra, tzu-mao li, rachit nigam, joshua tenenbaum, jonathan ragan-kelley","programming languages","often, a good explanation for a program's unexpected behavior is a bug in the programmer's code. but sometimes, an even better explanation is a bug in the programmer's mental model of the language they are using. instead of merely debugging our current code (""giving the programmer a fish""), what if our tools could directly debug our mental models (""teaching the programmer to fish"")? in this paper, we apply ideas from computational cognitive science to do exactly that. given a perplexing program, we use program synthesis techniques to automatically infer potential misconceptions that might cause the user to be surprised by the program's behavior. by analyzing these misconceptions, we provide succinct, useful explanations of the program's behavior. our methods can even be inverted to synthesize pedagogical example programs for diagnosing and correcting misconceptions in students.",2024-03-08
"llm4decompile: decompiling binary code with large language models","hanzhuo tan, qi luo, jing li, yuqun zhang","programming languages","decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. large language models (llms) show promise for programming tasks, motivating their application to decompilation. however, there does not exist any open-source llm for decompilation. moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. therefore, we release the first open-access decompilation llms ranging from 1b to 33b pre-trained on 4 billion tokens of c source code and the corresponding assembly code. the open-source llms can serve as baselines for further development in the field. to ensure practical program evaluation, we introduce decompile-eval, the first dataset that considers re-compilability and re-executability for decompilation. the benchmark emphasizes the importance of evaluating the decompilation model from the perspective of program semantics. experiments indicate that our llm4decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over gpt-4. our code, dataset, and models are released at this https url",2024-03-08
"dt-sim: property-based testing for mpc security","mako bates, joseph p. near","cryptography and security","formal methods for guaranteeing that a protocol satisfies a cryptographic security definition have advanced substantially, but such methods are still labor intensive and the need remains for an automated tool that can positively identify an insecure protocol. in this work, we demonstrate that property-based testing, ""run it a bunch of times and see if it breaks"", is effective for detecting security bugs in secure protocols. we specifically target secure multi-party computation (mpc), because formal methods targeting this security definition for bit-model implementations are particularly difficult. using results from the literature for probabilistic programming languages and statistical inference, we devise a test that can detect various flaws in a bit-level implementation of an mpc protocol. the test is grey-box; it requires only transcripts of randomness consumed by the protocol and of the inputs, outputs, and messages. it successfully detects several different mistakes and biases introduced into two different implementations of the classic gmw protocol. applied to hundreds of randomly generated protocols, it identifies nearly all of them as insecure. we also include an analysis of the parameters of the test, and discussion of what makes detection of mpc (in)security difficult.",2024-03-08
"automating the information extraction from semi-structured interview transcripts","angelina parfenova","computation and language","this paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of bert embeddings and hdbscan clustering. we present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. this tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.",2024-03-07
"evaluation of llms on syntax-aware code fill-in-the-middle tasks","linyuan gong, sida wang, mostafa elhoushi, alvin cheung","computation and language","we introduce syntax-aware fill-in-the-middle (safim), a new benchmark for evaluating large language models (llms) on the code fill-in-the-middle (fim) task. this benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after april 2022 to minimize data contamination. safim provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across llms. our comprehensive evaluation of 15 llms shows that fim pretraining not only enhances fim proficiency but also improves left-to-right (l2r) inference using llms. our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. safim thus serves as a foundational platform for future research in effective pretraining strategies for code llms. the evaluation toolkit and dataset are available at this https url, and the leaderboard is available at this https url.",2024-03-07
"quantifying contamination in evaluating code generation capabilities of language models","martin riddell, ansong ni, arman cohan","software engineering","while large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. while recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of llms in programming contexts. in this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. in our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. we also conduct extensive analysis on the factors that affects model memorization and generalization, such as model size, problem difficulty, and question length. we release all resulting files from our matching pipeline for future research.",2024-03-06
"online training of large language models: learn while chatting","juhao liang, ziwei wang, zhuoheng ma, jianquan li, zhiyi zhang, xiangbo wu, benyou wang","computation and language","large language models(llms) have dramatically revolutionized the field of natural language processing(nlp), offering remarkable capabilities that have garnered widespread usage. however, existing interaction paradigms between llms and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. this inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. to overcome these challenges, this paper introduces a novel interaction paradigm-'online training using external interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as ai agents or online/offline knowledge bases.",2024-03-04
"qrtree -- decision tree dialect specification of qrscript","stefano scanzio, matteo rosani, mattia scamuzzi, gianluca cena","networking and internet architecture","this specification document specifies the syntax and semantics of qrtree, which is a specific dialect of qrscript particularly suited to represent decision trees without chance nodes. the term dialect identifies one of the possible sub-languages that can be encoded inside of an eqr code via qrscript. this specification will describe an intermediate representation of qrtree, made through a language derived by the three-address code. it will then define the transformation rules from the intermediate representation to a binary code. the latter is a binary representation called eqrtreebytecode. these rules can also be applied inversely to transform the eqrtreebytecode into the intermediate representation. this specification document will pay particular attention to the creation of a compact eqrtreebytecode, as the maximum number of bits that can be stored in a qr code is, at the time of writing, equal to 2953 bytes (in the case of qr code version 40 with a ""low"" error correction level).",2024-03-07
"cedar: a new language for expressive, fast, safe, and analyzable authorization (extended version)","joseph w. cutler, craig disselkoen, aaron eline, shaobo he, kyle headley, michael hicks, kesha hietala, eleftherios ioannidis, john kastner, anwar mamat, darin mcadams, matt mccutchen, neha rungta, emina torlak, andrew wells","programming languages","cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. rather than embed authorization logic in an application's code, developers can write that logic as cedar policies and delegate access decisions to cedar's evaluation engine. cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. cedar's policy structure enables access requests to be decided quickly. cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized permissions do not change. we have modeled cedar in the lean programming language, and used lean's proof assistant to prove important properties of cedar's design. we have implemented cedar in rust, and released it open-source. comparing cedar to two open-source languages, openfga and rego, we find (subjectively) that cedar has equally or more readable policies, but (objectively) performs far better.",2024-03-07
"message-observing sessions","ryan kavanagh, brigitte pientka","programming languages","we present most, a process language with message-observing session types. message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels. hence, most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way. we give most a semantic foundation using traces with binding, a semantic approach for compositionally reasoning about traces in the presence of name generation. we use this semantics to prove type soundness and compositionality for most processes. we see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes.",2024-03-07
"strong priority and determinacy in timed ccs","luigi liquori, michael mendler","programming languages","building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called ""sequentially constructive reduction"" which is designed to capture the essence of synchronous programming. the distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. in particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language esterel. in the technical setting of ccs extended by clocks and priorities, we prove for a large class of processes, which we call ""structurally coherent"" the confluence property for constructive reductions. we further show that under some syntactic restrictions, called ""pivotable"" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. this covers a strictly larger class of processes compared to those that are confluent in milner's classical theory of ccs without priorities.",2024-03-07
"towards automatic composition of asp programs from natural language specifications","manuel borroto, irfan kareem, francesco ricca","artificial intelligence","this paper moves the first step towards automating the composition of answer set programming (asp) specifications. in particular, the following contributions are provided: (i) a dataset focused on graph-related problem specifications, designed to develop and assess tools for asp automatic coding; (ii) a two-step architecture, implemented in the nl2asp tool, for generating asp programs from natural language specifications. nl2asp uses neural machine translation to transform natural language into controlled natural language (cnl) statements. subsequently, cnl statements are converted into asp code using the cnl2asp tool. an experiment confirms the viability of the approach.",2024-03-07
"feedback-generation for programming exercises with gpt-4","imen azaiz, natalie kiesler, sven strickroth","artificial intelligence","ever since large language models (llms) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. llms such as codex, gpt-3.5, and gpt 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. this paper explores the quality of gpt-4 turbo's generated output for prompts containing both the programming task specification and a student's submission as input. two assignments from an introductory programming course were selected, and gpt-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. the output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. compared to prior work and analyses of gpt-3.5, gpt-4 turbo shows notable improvements. for example, the output is more structured and consistent. gpt-4 turbo can also accurately identify invalid casing in student programs' output. in some cases, the feedback also includes the output of the student program. at the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. the present work increases our understanding of llms' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on gpt-4.",2024-03-07
"whodunit: classifying code as human authored or gpt-4 generated -- a case study on codechef problems","oseremen joy idialu, noble saji mathews, rungroj maipradit, joanne m. atlee, mei nagappan","software engineering","artificial intelligence (ai) assistants such as github copilot and chatgpt, built on large language models like gpt-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative ai models. such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit ai generated code as their own work. our research explores the viability of using code stylometry and machine learning to distinguish between gpt-4 generated and human-authored code. our dataset comprises human-authored solutions from codechef and ai-authored solutions generated by gpt-4. our classifier outperforms baselines, with an f1-score and auc-roc score of 0.91. a variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an f1-score and auc-roc score of 0.89. we also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. our study shows that code stylometry is a promising approach for distinguishing between gpt-4 generated code and human-authored code.",2024-03-06
"guiding enumerative program synthesis with large language models","yixuan li, julian parsert, elizabeth polgreen","artificial intelligence","pre-trained large language models (llms) are beginning to dominate the discourse around automatic code generation with natural language specifications. in contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. in this paper, we evaluate the abilities of llms to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. when one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an llm into a weighted probabilistic search. this allows the synthesizer to provide the llm with information about the progress of the enumerator, and the llm to provide the enumerator with syntactic guidance in an iterative loop. we evaluate our techniques on benchmarks from the syntax-guided synthesis (sygus) competition. we find that gpt-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the llm into an enumerative synthesis algorithm shows significant performance gains over both the llm and the enumerative synthesizer alone and the winning sygus competition tool.",2024-03-06
"identify critical nodes in complex network with large language models","jinzhu mao, dongyun zou, li sheng, siyi liu, chen gao, yue wang, yong li","social and information networks","identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. therefore, we propose an approach that empowers evolutionary algorithm (ea) with large language models (llms), to generate a function called ""score\_nodes"" which can further be used to identify crucial nodes based on their assigned scores. our model consists of three main components: manual initialization, population management, and llms-based evolution. it evolves from initial populations with a set of designed node scoring functions created manually. llms leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. these functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. extensive experiments demonstrate the excellent performance of our method, showcasing its strong generalization ability compared to other state-of-the-art algorithms. it can consistently and orderly generate diverse and efficient node scoring functions. all source codes and models that can reproduce all results in this work are publicly available at this link: \url{https://anonymous.4open.science/r/llm4cn-6520}",2024-03-01
"ircoder: intermediate representations make language models robust multilingual code generators","indraneil paul, jun luo, goran glavaš, iryna gurevych","artificial intelligence","code understanding and generation have fast become some of the most popular applications of language models (lms). nonetheless, research on multilingual aspects of code-lms (i.e., lms for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc lm adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. in particular, most mainstream code-lms have been pre-trained on source code files alone. in this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of code-lms and facilitate cross-lingual transfer.
to this end, we first compile sltrans, a parallel dataset consisting of nearly 4m self-contained source code files coupled with respective intermediate representations. next, starting from various base code-lms (ranging in size from 1.1b to 7.3b parameters), we carry out continued causal language modelling training on sltrans, forcing the code-lms to (1) learn the ir language and (2) align the ir constructs with respective constructs of various programming languages. our resulting models, dubbed ircoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.",2024-03-06
"digitality as a ""longue durèe"" historical phenomenon","salvatore spina","computers and society","the digital age introduced the digital ecological niche (den), revolutionizing human interactions. the advent of digital history (dhy) has marked a methodological shift in historical studies, tracing its roots to babbage and lovelace's 19th-century work on ""coding"" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed ""person2persons2machines."" this evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by babbage and lovelace's contributions to mathematical linguistic systems, laying the groundwork for computer science. this field, central to 20th-century mainframe interaction through programming languages and formalization, situates digital history within a broader historical context. here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. nonetheless, the extent to which computation and turing machines can fully understand and interpret history remains a subject of debate.",2024-03-06
"robust mitl planning under uncertain navigation times","alexis linard, anna gautier, daniel duberg, jana tumova","robotics","in environments like offices, the duration of a robot's navigation between two locations may vary over time. for instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. in this work, we address the problem of routing in such environments with tasks expressed in metric interval temporal logic (mitl) - a rich robot task specification language that allows us to capture explicit time requirements. our objective is to find a strategy that maximizes the temporal robustness of the robot's mitl task. as the first step towards a solution, we define a mixed-integer linear programming approach to solving the task planning problem over a varying weighted transition system, where navigation durations are deterministic but vary depending on the time of day. then, we apply this planner to optimize for mitl temporal robustness in markov decision processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. finally, we develop a receding horizon planner for markov decision processes that preserves guarantees over mitl temporal robustness. we show the scalability of our planning algorithms in simulations of robotic tasks.",2024-03-06
"automatic bi-modal question title generation for stack overflow with prompt learning","shaoyu yang, xiang chen, ke liu, guang yang, chi yu","software engineering","when drafting question posts for stack overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. therefore, improving the quality of question titles has attracted the wide attention of researchers. an initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. however, this study ignored the helpful information in their corresponding problem descriptions. therefore, we propose an approach sotitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. later we fine-tune the pre-trained language model codet5 to automatically generate the titles. unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. to solve this issue, sotitle+ further prompt-tunes codet5 with hybrid prompts (i.e., mixture of hard and soft prompts). to verify the effectiveness of sotitle+, we construct a large-scale high-quality corpus from recent data dumps shared by stack overflow. our corpus includes 179,119 high-quality question posts for six popular programming languages. experimental results show that sotitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. our work indicates that considering bi-modal information and prompt learning in stack overflow title generation is a promising exploration direction.",2024-03-06
"magic markup: maintaining document-external markup with an llm","edward misback, zachary tatlock, steven l. tanimoto","computation and language","text documents, including programs, typically have human-readable semantic structure. historically, programmatic access to these semantics has required explicit in-document tagging. especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly. today, language models offer a new method: metadata can be bound to entities in changing text using a model's human-like understanding of semantics, with no requirements on the document structure. this method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation. we contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves. we also contribute a formal problem definition, an empirical synthetic benchmark suite, and our benchmark generator. our system achieves an accuracy of 90% on our benchmarks and can replace a document's tags in parallel at a rate of 5 seconds per tag. while there remains significant room for improvement, we find performance reliable enough to justify further exploration of applications.",2024-03-06
"generative explanations for program synthesizers","amirmohammad nazari, souti chattopadhyay, swabha swayamdipta, mukund raghothaman","programming languages","despite great advances in program synthesis techniques, they remain algorithmic black boxes. although they guarantee that when synthesis is successful, the implementation satisfies the specification, they provide no additional information regarding how the implementation works or the manner in which the specification is realized. one possibility to answer these questions is to use large language models (llms) to construct human-readable explanations. unfortunately, experiments reveal that llms frequently produce nonsensical or misleading explanations when applied to the unidiomatic code produced by program synthesizers.
in this paper, we develop an approach to reliably augment the implementation with explanatory names. we recover fine-grained input-output data from the synthesis algorithm to enhance the prompt supplied to the llm, and use a combination of a program verifier and a second language model to validate the proposed explanations before presenting them to the user. together, these techniques massively improve the accuracy of the proposed names, from 24% to 79% respectively. through a pair of small user studies, we find that users significantly prefer the explanations produced by our technique (76% of responses indicating the appropriateness of the presenting names) to the baseline (with only 2% of responses approving of the suggestions), and that the proposed names measurably help users in understanding the synthesized implementation.",2024-03-06
"explaining genetic programming trees using large language models","paula maddigan, andrew lensen, bing xue","neural and evolutionary computing","genetic programming (gp) has the potential to generate explainable results, especially when used for dimensionality reduction. in this research, we investigate the potential of leveraging explainable ai (xai) and large language models (llms) like chatgpt to improve the interpretability of gp-based non-linear dimensionality reduction. our study introduces a novel xai dashboard named gp4nldr, the first approach to combine state-of-the-art gp with an llm-powered chatbot to provide comprehensive, user-centred explanations. we showcase the system's ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies. our study highlights the importance of prompt engineering in eliciting accurate and pertinent responses from llms. we also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in generative ai. our findings demonstrate its potential in advancing the explainability of gp algorithms. this opens the door for future research into explaining gp models with llms.",2024-03-06
"algorithmic syntactic causal identification","dhurim cakiqi, max a. little","artificial intelligence","causal identification in causal bayes nets (cbns) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. however, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on cbns. however, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. we show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. in this alternative axiomatization, we show how an unambiguous and clean distinction can be drawn between the general syntax of causal models and any specific semantic implementation of that causal model. this allows a purely syntactic algorithmic description of general causal identification by a translation of recent formulations of the general id algorithm through fixing. our description is given entirely in terms of the non-parametric admg structure specifying a causal model and the algebraic signature of the corresponding monoidal category, to which a sequence of manipulations is then applied so as to arrive at a modified monoidal category in which the desired, purely syntactic interventional causal model, is obtained. we use this idea to derive purely syntactic analogues of classical back-door and front-door causal adjustment, and illustrate an application to a more complex causal model.",2024-03-14
"enabling waypoint generation for collaborative robots using llms and mixed reality","cathy mengying fang, krzysztof zieliński, pattie maes, joe paradiso, bruce blumberg, mikkel baun kjærgaard","human-computer interaction","programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot's physical constraints. we propose a framework that simplifies robot deployment by allowing direct communication using natural language. it uses large language models (llm) for prompt processing, workspace understanding, and waypoint generation. it also employs augmented reality (ar) to provide visual feedback of the planned outcome. we showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).",2024-03-14
"bugs in large language models generated code","florian tambon, arghavan moradi dakhel, amin nikanjam, foutse khomh, michel c. desmarais, giuliano antoniol","software engineering","large language models (llms) for code have gained significant attention recently. they can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in software engineering (se), i.e., automatic code generation. similar to human-written code, llm-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. given the increasing adoption of llm-based code generation tools (e.g., github copilot) in se activities, it is critical to understand the characteristics of bugs contained in code generated by llms. this paper examines a sample of 333 bugs collected from code generated using three leading llms (i.e., codegen, pangu-coder, and codex) and identifies the following 10 distinctive bug patterns: misinterpretations, syntax error, silly mistake, prompt-biased code, missing corner case, wrong input type, hallucinated object, wrong attribute, incomplete generation, and non-prompted consideration. the bug patterns are presented in the form of a taxonomy. the identified bug patterns are validated using an online survey with 34 llm practitioners and researchers. the surveyed participants generally asserted the significance and prevalence of the bug patterns. researchers and practitioners can leverage these findings to develop effective quality assurance techniques for llm-generated code. this study sheds light on the distinctive characteristics of llm-generated code.",2024-03-13
"formalizing date arithmetic and statically detecting ambiguities for the law","raphaël monat, aymeric fromherz, denis merigoux","programming languages","legal expert systems routinely rely on date computations to determine the eligibility of a citizen to social benefits or whether an application has been filed on time. unfortunately, date arithmetic exhibits many corner cases, which are handled differently from one library to the other, making faithfully transcribing the law into code error-prone, and possibly leading to heavy financial and legal consequences for users. in this work, we aim to provide a solid foundation for date arithmetic working on days, months and years. we first present a novel, formal semantics for date computations, and formally establish several semantic properties through a mechanization in the f* proof assistant. building upon this semantics, we then propose a static analysis by abstract interpretation to automatically detect ambiguities in date computations. we finally integrate our approach in the catala language, a recent domain-specific language for formalizing computational law, and use it to analyze the catala implementation of the french housing benefits, leading to the discovery of several date-related ambiguities.",2024-03-13
"jaxbind: bind any function to jax","jakob roth, martin reinecke, gordian edenhofer","instrumentation and methods for astrophysics","jax is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into jax. reimplementing the existing code in jax is often impractical and the existing interface in jax for binding custom code requires deep knowledge of jax and its c++ backend. the goal of jaxbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to jax. specifically, jaxbind provides an easy-to-use python interface for defining custom so-called jax primitives that support arbitrary jax transformations.",2024-03-13
"predictive analysis of tuberculosis treatment outcomes using machine learning: a karnataka tb data study at a scale","seshasai nath chinagudaba, darshan gera, krishna kiran vamsi dasu, uma shankar s, kiran k, anil singarajpure, shivayogappa.u, somashekar n, vineet kumar chadda, sharath b n","machine learning","tuberculosis (tb) remains a global health threat, ranking among the leading causes of mortality worldwide. in this context, machine learning (ml) has emerged as a transformative force, providing innovative solutions to the complexities associated with tb treatment.this study explores how machine learning, especially with tabular data, can be used to predict tuberculosis (tb) treatment outcomes more accurately. it transforms this prediction task into a binary classification problem, generating risk scores from patient data sourced from nikshay, india's national tb control program, which includes over 500,000 patient records.
data preprocessing is a critical component of the study, and the model achieved an recall of 98% and an auc-roc score of 0.95 on the validation set, which includes 20,000 patient records.we also explore the use of natural language processing (nlp) for improved model learning. our results, corroborated by various metrics and ablation studies, validate the effectiveness of our approach. the study concludes by discussing the potential ramifications of our research on tb eradication efforts and proposing potential avenues for future work. this study marks a significant stride in the battle against tb, showcasing the potential of machine learning in healthcare.",2024-03-13
"devbench: a comprehensive benchmark for software development","bowen li, wenhan wu, ziwei tang, lin shi, john yang, jinyang li, shunyu yao, chen qian, binyuan hui, qicheng zhang, zhiyin yu, he du, ping yang, dahua lin, chao peng, kai chen","computation and language","recent advancements in large language models (llms) have significantly enhanced their coding capabilities. however, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. to this end, we propose devbench, a comprehensive benchmark that evaluates llms across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. devbench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. empirical studies show that current llms, including gpt-4-turbo, fail to solve the challenges presented within devbench. analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. our findings offer actionable insights for the future development of llms toward real-world programming applications. our benchmark is available at this https url",2024-03-13
"a picture is worth a thousand words: exploring diagram and video-based oop exercises to counter llm over-reliance","bruno pereira cipriano, pedro alves, paul denny","software engineering","much research has highlighted the impressive capabilities of large language models (llms), like gpt and bard, for solving introductory programming exercises. recent work has shown that llms can effectively solve a range of more complex object-oriented programming (oop) exercises with text-based specifications. this raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. to address this, we propose an innovative approach to formulating oop tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in oop courses. we introduce a novel notation system for specifying oop assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. student perceptions of this approach are explored through a survey (n=56). generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. this notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. furthermore, students reported being less inclined to rely on llm-based code generation tools for these diagram and video-based exercises. experiments with gpt-4 and bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions.",2024-03-13
"cleanagent: automating data standardization with llm-based agents","danrui qi, jiannan wang","machine learning","data standardization is a crucial part in data science life cycle. while tools like pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. although large language models (llms) like chatgpt have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. to solve these challenges, our key idea is to propose a python library with declarative, unified apis for standardizing column types, simplifying the code generation of llm with concise api calls. we first propose dataprep.clean which is written as a component of the dataprep library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. then we introduce the cleanagent framework integrating dataprep.clean and llm-based agents to automate the data standardization process. with cleanagent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.",2024-03-13
"mastering text, code and math simultaneously via fusing highly specialized language models","ning ding, yulin chen, ganqu cui, xingtai lv, ruobing xie, bowen zhou, zhiyuan liu, maosong sun","computation and language","underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (llms) that strive to achieve high performance across all three domains simultaneously. achieving a very high level of proficiency for an llm within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. in this paper, we propose to fuse models that are already highly-specialized directly. the proposed fusing framework, ultrafuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. a token-level gating mechanism is introduced to blend the specialists' outputs. a two-stage training strategy accompanied by balanced sampling is designed to ensure stability. to effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, ultrachat 2, which includes text, code, and mathematical content. this dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. experiments show that our model could simultaneously achieve mastery of the three crucial domains.",2024-03-13
"continuous object state recognition for cooking robots using pre-trained vision-language models and black-box optimization","kento kawaharazuka, naoaki kanazawa, yoshiki obinata, kei okada, masayuki inaba","robotics","the state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. on the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. in addition, the state changes of food are complex and cannot be easily described by manual programming. therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. by using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. we also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved. we demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying.",2024-03-13
"empowering robotics with large language models: osmag map comprehension with llms","fujing xie, sören schwertfeger","robotics","recently, large language models (llms) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. in this letter, we address the problem of enabling llms to comprehend area graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics. area graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. in contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmag (area graph in opensstreetmap format) is stored in a xml textual format naturally readable by llms. furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmag, facilitating this map representation comprehensible by llms, traditional robotic algorithms and humans. our experiments show that with a proper map representation, llms possess the capability to understand maps and answer queries based on that understanding. following simple fine-tuning of llama2 models, it surpassed chatgpt-3.5 in tasks involving topology and hierarchy understanding. our dataset, dataset generation code, fine-tuned lora adapters can be accessed at this https url.",2024-03-13
"merino: entropy-driven design for generative language models on iot devices","youpeng zhao, ming lin, huadong tang, qiang wu, jun wang","machine learning","generative large language models (llms) stand as a revolutionary advancement in the modern era of artificial intelligence (ai). however, directly deploying llms in resource-constrained hardware, such as internet-of-things (iot) devices, is difficult due to their high computational cost. in this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. the whole design procedure involves solving a mathematical programming (mp) problem, which can be done on the cpu within minutes, making it nearly zero-cost. we evaluate our designed models, termed merino, across nine nlp downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. notably, merino achieves similar or better zero performance compared to the 350m parameter opt while being 4.9x faster on nvidia jetson nano with 5.5x reduction in model size. code will be made available soon.",2024-02-28
"neural slot interpreters: grounding object semantics in emergent slot representations","bhishma dedhia, niraj k. jha","computer vision and pattern recognition","object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. however, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. we present the neural slot interpreter (nsi) that learns to ground and generate object semantics via slot representations. at the core of nsi is an xml-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. finally, we formulate the nsi program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. experiments on bi-modal retrieval tasks demonstrate the efficacy of the learned alignments, surpassing set-matching-based predictors by a significant margin. moreover, learning the program generator from grounded associations enhances the predictive power of slots. nsi generated programs demonstrate improved performance of object-centric learners on property prediction and object detection, and scale with real-world scene complexity.",2024-02-02
"exploring safety generalization challenges of large language models via code","qibing ren, chang gao, jing shao, junchi yan, xin tan, yu qiao, wai lam, lizhuang ma","computation and language","the rapid advancement of large language models (llms) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. while strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. this paper introduces codeattack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of llms. our comprehensive studies on state-of-the-art llms including gpt-4, claude-2, and llama-2 series reveal a common safety vulnerability of these models against code input: codeattack consistently bypasses the safety guardrails of all models more than 80% of the time. furthermore, we find that a larger distribution gap between codeattack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. these findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of llms.",2024-03-12
"proskill: a formal skill language for acting in robotics","félix ingrand (laas-cnrs, université de toulouse, toulouse, france)","robotics","acting is an important decisional function for autonomous robots. acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. while sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. this language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. we illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission.",2024-03-12
"couler: unified machine learning workflow optimization in cloud","xiaoda wang, yuan tang, tengda guo, bo sang, jingji wu, jian sha, ke zhang, jiang qian, mingjie tang","databases","machine learning (ml) has become ubiquitous, fueling data-driven applications across various organizations. contrary to the traditional perception of ml in research, ml workflows can be complex, resource-intensive, and time-consuming. expanding an ml workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. currently, numerous workflow engines are available (with over ten being widely recognized). this variety poses a challenge for end-users in terms of mastering different engine apis. while efforts have primarily focused on optimizing ml operations (mlops) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.
in this work, we design and implement couler, a system designed for unified ml workflow optimization in the cloud. our main insight lies in the ability to generate an ml workflow using natural language (nl) descriptions. we integrate large language models (llms) into workflow generation, and provide a unified programming interface for various workflow engines. this approach alleviates the need to understand various workflow engines' apis. moreover, couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. these enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. couler is extensively deployed in real-world production scenarios at ant group, handling approximately 22k workflows daily, and has successfully improved the cpu/memory utilization by more than 15% and the workflow completion rate by around 17%.",2024-03-12
"drplanner: diagnosis and repair of motion planners using large language models","yuanfei lin, chenran li, mingyu ding, masayoshi tomizuka, wei zhan, matthias althoff","robotics","motion planners are essential for the safe operation of automated vehicles across various scenarios. however, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. to tackle the aforementioned issues, we present drplanner, the first framework designed to automatically diagnose and repair motion planners using large language models. initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively.",2024-03-12
"from english to asic: hardware implementation with large language model","emil goh, maoyang xiang, i-chyn wey, t. hui teo","hardware architecture","in the realm of asic engineering, the landscape has been significantly reshaped by the rapid development of llm, paralleled by an increase in the complexity of modern digital circuits. this complexity has escalated the requirements for hdl coding, necessitating a higher degree of precision and sophistication. however, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. these challenges have highlighted the gap between the potential of llms to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. to address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the hdl code dataset has been developed. the fine-tuning aims to enhance models' proficiency in generating precise and efficient asic design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. the model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. this approach is expected to facilitate a simplified and more efficient llm-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of hdl coding and thus streamlining the asic development process.",2024-03-11
"deriving dependently-typed oop from first principles -- extended version with additional appendices","david binder, ingo skupin, tim süberkrüb, klaus ostermann","programming languages","the expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. when abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. the expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). this difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. in this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. that is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. our central contribution is a dependently typed calculus which contains two dual language fragments. we provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. we have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality.",2024-03-11
"automatic generation of python programs using context-free grammars","kamel yamani, marwa naïr, riyadh baghdadi","programming languages","in recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. however, procuring high-quality data remains challenging, especially for code. to address this, we developed tinypy generator, a tool that generates random python programs using a context-free grammar. the generated programs are guaranteed to be correct by construction. our system uses custom production rules (in the backus-naur form (bnf) format) to recursively generate code. this allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. our proposed tool enables effortless large-scale python code generation, beneficial for a wide range of applications. tinypy generator is particularly useful in the field of machine learning, where it can generate substantial amounts of python code for training python language models. additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of code interpreters or compilers. unlike existing research, we have open-sourced our implementation. this allows customization according to user needs and extends potential usage to other languages.",2024-03-11
"llms still can't avoid instanceof: an investigation into gpt-3.5, gpt-4 and bard's capacity to handle object-oriented programming assignments","bruno pereira cipriano, pedro alves","software engineering","large language models (llms) have emerged as promising tools to assist students while solving programming assignments. however, object-oriented programming (oop), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. contrary to introductory programming exercises, there exists a research gap with regard to the behavior of llms in oop contexts. in this study, we experimented with three prominent llms - gpt-3.5, gpt-4, and bard - to solve real-world oop exercises used in educational settings, subsequently validating their solutions using an automatic assessment tool (aat). the findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of oop. gpt-4 stood out as the most proficient, followed by gpt-3.5, with bard trailing last. we advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing llms with aats in pedagogical settings. in conclusion, while gpt-4 showcases promise, the deployment of these models in oop education still mandates supervision.",2024-03-10
"are llms ready for visualization?","pere-pau vázquez","human-computer interaction","generative models have received a lot of attention in many areas of academia and the industry. their capabilities span many areas, from the invention of images given a prompt to the generation of concrete code to solve a certain programming issue. these two paradigmatic cases fall within two distinct categories of requirements, ranging from ""creativity"" to ""precision"", as characterized by bing chat, which employs chatgpt-4 as its backbone. visualization practitioners and researchers have wondered to what end one of such systems could accomplish our work in a more efficient way. several works in the literature have utilized them for the creation of visualizations. and some tools such as lida, incorporate them as part of their pipeline. nevertheless, to the authors' knowledge, no systematic approach for testing their capabilities has been published, which includes both extensive and in-depth evaluation. our goal is to fill that gap with a systematic approach that analyzes three elements: whether large language models are capable of correctly generating a large variety of charts, what libraries they can deal with effectively, and how far we can go to configure individual charts. to achieve this objective, we initially selected a diverse set of charts, which are commonly utilized in data visualization. we then developed a set of generic prompts that could be used to generate them, and analyzed the performance of different llms and libraries. the results include both the set of prompts and the data sources, as well as an analysis of the performance with different configurations.",2024-03-10
"explaining code with a purpose: an integrated approach for developing code comprehension and prompting skills","paul denny, david h. smith iv, max fowler, james prather, brett a. becker, juho leinonen","human-computer interaction","reading, understanding and explaining code have traditionally been important skills for novices learning programming. as large language models (llms) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an llm. thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with llms. one effective way to develop and assess code comprehension ability is with ``explain in plain english'' (eipe) questions, where students succinctly explain the purpose of a fragment of code. however, grading eipe questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. in this paper, we explore a natural synergy between eipe questions and code-generating llms to overcome this limitation. we propose using an llm to generate code based on students' responses to eipe questions -- not only enabling eipe responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. we investigate this idea in an introductory programming course and report student success in creating effective prompts for solving eipe questions. we also examine student perceptions of this activity and how it influences their views on the use of llms for aiding and assessing learning.",2024-03-10
"a novel refactoring and semantic aware abstract syntax tree differencing tool and a benchmark for evaluating the accuracy of diff tools","pouria alikhanifard, nikolaos tsantalis","software engineering","software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. abstract syntax tree (ast) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. despite the notable improvements brought by ast diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible ast nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. we propose a novel ast diff tool based on refactoringminer that resolves all aforementioned limitations. first, we improved refactoringminer to increase its statement mapping accuracy, and then we developed an algorithm that generates ast diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by refactoringminer. to evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of ast node mappings, including 800 bug-fixing commits and 188 refactoring commits. our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.",2024-03-09
"unisparse: an intermediate language for general sparse format customization","jie liu, zhongyuan zhao, zijian ding, benjamin brock, hongbo rong, zhiru zhang","computation and language","the ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. these formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. however, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. to overcome this deficiency, we propose unisparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. unlike the existing attribute-based frameworks, unisparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. as a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. we also develop a compiler leveraging the mlir infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. we demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an intel cpu, an nvidia gpu, an amd xilinx fpga, and a simulated processing-in-memory (pim) device.",2024-03-09
"we know i know you know; choreographic programming with multicast and multiply located values","mako bates, joseph p. near","programming languages","concurrent distributed systems are notoriously difficult to construct and reason about. choreographic programming is a recent paradigm that describes a distributed system in a single global program called a choreography. choreographies simplify reasoning about distributed systems and can ensure deadlock freedom by static analysis. in previous choreographic programming languages, each value is located at a single party, and the programmer is expected to insert special untyped ""select"" operations to ensure that all parties follow the same communication pattern.
we present he-lambda-small, a new choreographic programming language with multiply located values. he-lambda-small allows multicasting to a set of parties, and the resulting value will be located at all of them. this approach enables a simple and elegant alternative to ""select"": he-lambda-small requires that the guard for a conditional be located at all of the relevant parties. in he-lambda-small, checking that a choreography is well-typed suffices to show that it is deadlock-free. we present several case studies that demonstrate the use of multiply-located values to concisely encode tricky communication patterns described in previous work without the use of ""select"" or redundant communication.",2024-03-08
"watchat: explaining perplexing programs by debugging mental models","kartik chandra, tzu-mao li, rachit nigam, joshua tenenbaum, jonathan ragan-kelley","programming languages","often, a good explanation for a program's unexpected behavior is a bug in the programmer's code. but sometimes, an even better explanation is a bug in the programmer's mental model of the language they are using. instead of merely debugging our current code (""giving the programmer a fish""), what if our tools could directly debug our mental models (""teaching the programmer to fish"")? in this paper, we apply ideas from computational cognitive science to do exactly that. given a perplexing program, we use program synthesis techniques to automatically infer potential misconceptions that might cause the user to be surprised by the program's behavior. by analyzing these misconceptions, we provide succinct, useful explanations of the program's behavior. our methods can even be inverted to synthesize pedagogical example programs for diagnosing and correcting misconceptions in students.",2024-03-08
"llm4decompile: decompiling binary code with large language models","hanzhuo tan, qi luo, jing li, yuqun zhang","programming languages","decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. large language models (llms) show promise for programming tasks, motivating their application to decompilation. however, there does not exist any open-source llm for decompilation. moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. therefore, we release the first open-access decompilation llms ranging from 1b to 33b pre-trained on 4 billion tokens of c source code and the corresponding assembly code. the open-source llms can serve as baselines for further development in the field. to ensure practical program evaluation, we introduce decompile-eval, the first dataset that considers re-compilability and re-executability for decompilation. the benchmark emphasizes the importance of evaluating the decompilation model from the perspective of program semantics. experiments indicate that our llm4decompile has demonstrated the capability to accurately decompile 21% of the assembly code, which achieves a 50% improvement over gpt-4. our code, dataset, and models are released at this https url",2024-03-08
"dt-sim: property-based testing for mpc security","mako bates, joseph p. near","cryptography and security","formal methods for guaranteeing that a protocol satisfies a cryptographic security definition have advanced substantially, but such methods are still labor intensive and the need remains for an automated tool that can positively identify an insecure protocol. in this work, we demonstrate that property-based testing, ""run it a bunch of times and see if it breaks"", is effective for detecting security bugs in secure protocols. we specifically target secure multi-party computation (mpc), because formal methods targeting this security definition for bit-model implementations are particularly difficult. using results from the literature for probabilistic programming languages and statistical inference, we devise a test that can detect various flaws in a bit-level implementation of an mpc protocol. the test is grey-box; it requires only transcripts of randomness consumed by the protocol and of the inputs, outputs, and messages. it successfully detects several different mistakes and biases introduced into two different implementations of the classic gmw protocol. applied to hundreds of randomly generated protocols, it identifies nearly all of them as insecure. we also include an analysis of the parameters of the test, and discussion of what makes detection of mpc (in)security difficult.",2024-03-08
"automating the information extraction from semi-structured interview transcripts","angelina parfenova","computation and language","this paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of bert embeddings and hdbscan clustering. we present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. this tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.",2024-03-07
"evaluation of llms on syntax-aware code fill-in-the-middle tasks","linyuan gong, sida wang, mostafa elhoushi, alvin cheung","computation and language","we introduce syntax-aware fill-in-the-middle (safim), a new benchmark for evaluating large language models (llms) on the code fill-in-the-middle (fim) task. this benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after april 2022 to minimize data contamination. safim provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across llms. our comprehensive evaluation of 15 llms shows that fim pretraining not only enhances fim proficiency but also improves left-to-right (l2r) inference using llms. our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. safim thus serves as a foundational platform for future research in effective pretraining strategies for code llms. the evaluation toolkit and dataset are available at this https url, and the leaderboard is available at this https url.",2024-03-07
"quantifying contamination in evaluating code generation capabilities of language models","martin riddell, ansong ni, arman cohan","software engineering","while large language models have achieved remarkable performance on various code generation benchmarks, there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data. while recent work has investigated contamination in natural language generation and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of code generation, which is critical for understanding the robustness and reliability of llms in programming contexts. in this work, we perform a comprehensive study of data contamination of popular code generation benchmarks, and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. in our experiments, we show that there are substantial overlap between popular code generation benchmarks and open training corpus, and models perform significantly better on the subset of the benchmarks where similar solutions are seen during training. we also conduct extensive analysis on the factors that affects model memorization and generalization, such as model size, problem difficulty, and question length. we release all resulting files from our matching pipeline for future research.",2024-03-06
"online training of large language models: learn while chatting","juhao liang, ziwei wang, zhuoheng ma, jianquan li, zhiyi zhang, xiangbo wu, benyou wang","computation and language","large language models(llms) have dramatically revolutionized the field of natural language processing(nlp), offering remarkable capabilities that have garnered widespread usage. however, existing interaction paradigms between llms and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. this inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. to overcome these challenges, this paper introduces a novel interaction paradigm-'online training using external interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as ai agents or online/offline knowledge bases.",2024-03-04
"qrtree -- decision tree dialect specification of qrscript","stefano scanzio, matteo rosani, mattia scamuzzi, gianluca cena","networking and internet architecture","this specification document specifies the syntax and semantics of qrtree, which is a specific dialect of qrscript particularly suited to represent decision trees without chance nodes. the term dialect identifies one of the possible sub-languages that can be encoded inside of an eqr code via qrscript. this specification will describe an intermediate representation of qrtree, made through a language derived by the three-address code. it will then define the transformation rules from the intermediate representation to a binary code. the latter is a binary representation called eqrtreebytecode. these rules can also be applied inversely to transform the eqrtreebytecode into the intermediate representation. this specification document will pay particular attention to the creation of a compact eqrtreebytecode, as the maximum number of bits that can be stored in a qr code is, at the time of writing, equal to 2953 bytes (in the case of qr code version 40 with a ""low"" error correction level).",2024-03-07
"cedar: a new language for expressive, fast, safe, and analyzable authorization (extended version)","joseph w. cutler, craig disselkoen, aaron eline, shaobo he, kyle headley, michael hicks, kesha hietala, eleftherios ioannidis, john kastner, anwar mamat, darin mcadams, matt mccutchen, neha rungta, emina torlak, andrew wells","programming languages","cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. rather than embed authorization logic in an application's code, developers can write that logic as cedar policies and delegate access decisions to cedar's evaluation engine. cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. cedar's policy structure enables access requests to be decided quickly. cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized permissions do not change. we have modeled cedar in the lean programming language, and used lean's proof assistant to prove important properties of cedar's design. we have implemented cedar in rust, and released it open-source. comparing cedar to two open-source languages, openfga and rego, we find (subjectively) that cedar has equally or more readable policies, but (objectively) performs far better.",2024-03-07
"message-observing sessions","ryan kavanagh, brigitte pientka","programming languages","we present most, a process language with message-observing session types. message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels. hence, most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way. we give most a semantic foundation using traces with binding, a semantic approach for compositionally reasoning about traces in the presence of name generation. we use this semantics to prove type soundness and compositionality for most processes. we see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes.",2024-03-07
"strong priority and determinacy in timed ccs","luigi liquori, michael mendler","programming languages","building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called ""sequentially constructive reduction"" which is designed to capture the essence of synchronous programming. the distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. in particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language esterel. in the technical setting of ccs extended by clocks and priorities, we prove for a large class of processes, which we call ""structurally coherent"" the confluence property for constructive reductions. we further show that under some syntactic restrictions, called ""pivotable"" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. this covers a strictly larger class of processes compared to those that are confluent in milner's classical theory of ccs without priorities.",2024-03-07
"towards automatic composition of asp programs from natural language specifications","manuel borroto, irfan kareem, francesco ricca","artificial intelligence","this paper moves the first step towards automating the composition of answer set programming (asp) specifications. in particular, the following contributions are provided: (i) a dataset focused on graph-related problem specifications, designed to develop and assess tools for asp automatic coding; (ii) a two-step architecture, implemented in the nl2asp tool, for generating asp programs from natural language specifications. nl2asp uses neural machine translation to transform natural language into controlled natural language (cnl) statements. subsequently, cnl statements are converted into asp code using the cnl2asp tool. an experiment confirms the viability of the approach.",2024-03-07
"feedback-generation for programming exercises with gpt-4","imen azaiz, natalie kiesler, sven strickroth","artificial intelligence","ever since large language models (llms) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. llms such as codex, gpt-3.5, and gpt 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. this paper explores the quality of gpt-4 turbo's generated output for prompts containing both the programming task specification and a student's submission as input. two assignments from an introductory programming course were selected, and gpt-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. the output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. compared to prior work and analyses of gpt-3.5, gpt-4 turbo shows notable improvements. for example, the output is more structured and consistent. gpt-4 turbo can also accurately identify invalid casing in student programs' output. in some cases, the feedback also includes the output of the student program. at the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. the present work increases our understanding of llms' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on gpt-4.",2024-03-07
"whodunit: classifying code as human authored or gpt-4 generated -- a case study on codechef problems","oseremen joy idialu, noble saji mathews, rungroj maipradit, joanne m. atlee, mei nagappan","software engineering","artificial intelligence (ai) assistants such as github copilot and chatgpt, built on large language models like gpt-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative ai models. such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit ai generated code as their own work. our research explores the viability of using code stylometry and machine learning to distinguish between gpt-4 generated and human-authored code. our dataset comprises human-authored solutions from codechef and ai-authored solutions generated by gpt-4. our classifier outperforms baselines, with an f1-score and auc-roc score of 0.91. a variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an f1-score and auc-roc score of 0.89. we also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. our study shows that code stylometry is a promising approach for distinguishing between gpt-4 generated code and human-authored code.",2024-03-06
"guiding enumerative program synthesis with large language models","yixuan li, julian parsert, elizabeth polgreen","artificial intelligence","pre-trained large language models (llms) are beginning to dominate the discourse around automatic code generation with natural language specifications. in contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. in this paper, we evaluate the abilities of llms to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. when one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an llm into a weighted probabilistic search. this allows the synthesizer to provide the llm with information about the progress of the enumerator, and the llm to provide the enumerator with syntactic guidance in an iterative loop. we evaluate our techniques on benchmarks from the syntax-guided synthesis (sygus) competition. we find that gpt-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the llm into an enumerative synthesis algorithm shows significant performance gains over both the llm and the enumerative synthesizer alone and the winning sygus competition tool.",2024-03-06
"identify critical nodes in complex network with large language models","jinzhu mao, dongyun zou, li sheng, siyi liu, chen gao, yue wang, yong li","social and information networks","identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. therefore, we propose an approach that empowers evolutionary algorithm (ea) with large language models (llms), to generate a function called ""score\_nodes"" which can further be used to identify crucial nodes based on their assigned scores. our model consists of three main components: manual initialization, population management, and llms-based evolution. it evolves from initial populations with a set of designed node scoring functions created manually. llms leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. these functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. extensive experiments demonstrate the excellent performance of our method, showcasing its strong generalization ability compared to other state-of-the-art algorithms. it can consistently and orderly generate diverse and efficient node scoring functions. all source codes and models that can reproduce all results in this work are publicly available at this link: \url{https://anonymous.4open.science/r/llm4cn-6520}",2024-03-01
"ircoder: intermediate representations make language models robust multilingual code generators","indraneil paul, jun luo, goran glavaš, iryna gurevych","artificial intelligence","code understanding and generation have fast become some of the most popular applications of language models (lms). nonetheless, research on multilingual aspects of code-lms (i.e., lms for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc lm adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. in particular, most mainstream code-lms have been pre-trained on source code files alone. in this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of code-lms and facilitate cross-lingual transfer.
to this end, we first compile sltrans, a parallel dataset consisting of nearly 4m self-contained source code files coupled with respective intermediate representations. next, starting from various base code-lms (ranging in size from 1.1b to 7.3b parameters), we carry out continued causal language modelling training on sltrans, forcing the code-lms to (1) learn the ir language and (2) align the ir constructs with respective constructs of various programming languages. our resulting models, dubbed ircoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.",2024-03-06
"digitality as a ""longue durèe"" historical phenomenon","salvatore spina","computers and society","the digital age introduced the digital ecological niche (den), revolutionizing human interactions. the advent of digital history (dhy) has marked a methodological shift in historical studies, tracing its roots to babbage and lovelace's 19th-century work on ""coding"" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed ""person2persons2machines."" this evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by babbage and lovelace's contributions to mathematical linguistic systems, laying the groundwork for computer science. this field, central to 20th-century mainframe interaction through programming languages and formalization, situates digital history within a broader historical context. here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. nonetheless, the extent to which computation and turing machines can fully understand and interpret history remains a subject of debate.",2024-03-06
"robust mitl planning under uncertain navigation times","alexis linard, anna gautier, daniel duberg, jana tumova","robotics","in environments like offices, the duration of a robot's navigation between two locations may vary over time. for instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. in this work, we address the problem of routing in such environments with tasks expressed in metric interval temporal logic (mitl) - a rich robot task specification language that allows us to capture explicit time requirements. our objective is to find a strategy that maximizes the temporal robustness of the robot's mitl task. as the first step towards a solution, we define a mixed-integer linear programming approach to solving the task planning problem over a varying weighted transition system, where navigation durations are deterministic but vary depending on the time of day. then, we apply this planner to optimize for mitl temporal robustness in markov decision processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. finally, we develop a receding horizon planner for markov decision processes that preserves guarantees over mitl temporal robustness. we show the scalability of our planning algorithms in simulations of robotic tasks.",2024-03-06
"automatic bi-modal question title generation for stack overflow with prompt learning","shaoyu yang, xiang chen, ke liu, guang yang, chi yu","software engineering","when drafting question posts for stack overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. therefore, improving the quality of question titles has attracted the wide attention of researchers. an initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. however, this study ignored the helpful information in their corresponding problem descriptions. therefore, we propose an approach sotitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. later we fine-tune the pre-trained language model codet5 to automatically generate the titles. unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. to solve this issue, sotitle+ further prompt-tunes codet5 with hybrid prompts (i.e., mixture of hard and soft prompts). to verify the effectiveness of sotitle+, we construct a large-scale high-quality corpus from recent data dumps shared by stack overflow. our corpus includes 179,119 high-quality question posts for six popular programming languages. experimental results show that sotitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. our work indicates that considering bi-modal information and prompt learning in stack overflow title generation is a promising exploration direction.",2024-03-06
"magic markup: maintaining document-external markup with an llm","edward misback, zachary tatlock, steven l. tanimoto","computation and language","text documents, including programs, typically have human-readable semantic structure. historically, programmatic access to these semantics has required explicit in-document tagging. especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly. today, language models offer a new method: metadata can be bound to entities in changing text using a model's human-like understanding of semantics, with no requirements on the document structure. this method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation. we contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves. we also contribute a formal problem definition, an empirical synthetic benchmark suite, and our benchmark generator. our system achieves an accuracy of 90% on our benchmarks and can replace a document's tags in parallel at a rate of 5 seconds per tag. while there remains significant room for improvement, we find performance reliable enough to justify further exploration of applications.",2024-03-06
"generative explanations for program synthesizers","amirmohammad nazari, souti chattopadhyay, swabha swayamdipta, mukund raghothaman","programming languages","despite great advances in program synthesis techniques, they remain algorithmic black boxes. although they guarantee that when synthesis is successful, the implementation satisfies the specification, they provide no additional information regarding how the implementation works or the manner in which the specification is realized. one possibility to answer these questions is to use large language models (llms) to construct human-readable explanations. unfortunately, experiments reveal that llms frequently produce nonsensical or misleading explanations when applied to the unidiomatic code produced by program synthesizers.
in this paper, we develop an approach to reliably augment the implementation with explanatory names. we recover fine-grained input-output data from the synthesis algorithm to enhance the prompt supplied to the llm, and use a combination of a program verifier and a second language model to validate the proposed explanations before presenting them to the user. together, these techniques massively improve the accuracy of the proposed names, from 24% to 79% respectively. through a pair of small user studies, we find that users significantly prefer the explanations produced by our technique (76% of responses indicating the appropriateness of the presenting names) to the baseline (with only 2% of responses approving of the suggestions), and that the proposed names measurably help users in understanding the synthesized implementation.",2024-03-06
"explaining genetic programming trees using large language models","paula maddigan, andrew lensen, bing xue","neural and evolutionary computing","genetic programming (gp) has the potential to generate explainable results, especially when used for dimensionality reduction. in this research, we investigate the potential of leveraging explainable ai (xai) and large language models (llms) like chatgpt to improve the interpretability of gp-based non-linear dimensionality reduction. our study introduces a novel xai dashboard named gp4nldr, the first approach to combine state-of-the-art gp with an llm-powered chatbot to provide comprehensive, user-centred explanations. we showcase the system's ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies. our study highlights the importance of prompt engineering in eliciting accurate and pertinent responses from llms. we also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in generative ai. our findings demonstrate its potential in advancing the explainability of gp algorithms. this opens the door for future research into explaining gp models with llms.",2024-03-06
